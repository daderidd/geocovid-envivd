{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from contextlib import redirect_stdout\n",
    "import multiprocessing as mp\n",
    "import datetime\n",
    "import re\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "# Third-party imports\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from matplotlib import patches as mpatches\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from importlib import reload\n",
    "from typing import Union, Dict, List, Tuple\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pysal\n",
    "import esda\n",
    "import pysda\n",
    "import libpysal as lps\n",
    "from libpysal.weights.distance import get_points_array\n",
    "from mgwr.gwr import GWR, MGWR\n",
    "from mgwr.sel_bw import Sel_BW\n",
    "from mgwr.utils import shift_colormap, truncate_colormap, compare_surfaces\n",
    "\n",
    "import rasterio\n",
    "from rasterio.plot import show_hist, show\n",
    "from rasterio.mask import mask\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import statsmodels.stats.outliers_influence as oi\n",
    "\n",
    "import xgboost\n",
    "import shap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from geofeather import to_geofeather, from_geofeather\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "import folium\n",
    "from folium.plugins import Fullscreen\n",
    "\n",
    "import contextily as ctx\n",
    "\n",
    "# Local imports\n",
    "sys.path.append('./utils')\n",
    "import utils\n",
    "import pyspace\n",
    "\n",
    "\n",
    "# Configuration settings and setup code\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path('../data/')\n",
    "results_folder = Path('../results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cantons_ch = gpd.read_file(data_folder/'Administrative units'/'swissBOUNDARIES3D_1_3_TLM_KANTONSGEBIET.shp', engine='pyogrio')\n",
    "cantons_ch = cantons_ch.to_crs(2056)\n",
    "canton_vd = cantons_ch[(cantons_ch.NAME == 'Vaud')&(cantons_ch.EINWOHNERZ.isnull()==False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Load cluster persistence and determinants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_parquet(data_folder/'df_survival_covariates.geoparquet')\n",
    "df_period = gpd.read_parquet(data_folder/'df_survival_period_covariates.geoparquet')\n",
    "\n",
    "df_lausanne = gpd.read_parquet(data_folder/'df_survival_covariates_lausanne.geoparquet')\n",
    "df_period_lausanne = gpd.read_parquet(data_folder/'df_survival_period_covariates_lausanne.geoparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Process satellite imagery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data directory and file paths\n",
    "pollution_folder = data_folder/\"Pollution NO2/\"\n",
    "raster_file = \"download.tropospheric_NO2_column_number_density 1.tif\"\n",
    "statpop_points_file = \"statpop_points.feather\"\n",
    "statpop_ha_file = \"statpop_communes_vd_ha.feather\"\n",
    "\n",
    "# Load the GeoDataFrames\n",
    "statpop_points_gdf = from_geofeather(os.path.join(data_folder,'ag-b-00.03-vz2020statpop', statpop_points_file))\n",
    "statpop_ha_gdf = from_geofeather(os.path.join(data_folder, 'ag-b-00.03-vz2020statpop', statpop_ha_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"statpop_ha_with_NO2.feather\"\n",
    "output_path = os.path.join(pollution_folder, output_file)\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    # Process all raster data to obtain min, max, mean, median NO2 values by month and for each hectare of the canton of VD\n",
    "    all_stats = []\n",
    "    try:\n",
    "        for i in range(1, 45):\n",
    "            raster_file = f\"download.tropospheric_NO2_column_number_density {i}.tif\"\n",
    "            print(f\"Processing raster {i}\")\n",
    "            \n",
    "            stats, statpop_ha_gdf = utils.process_raster_data(\n",
    "                raster_path=raster_file,\n",
    "                index=i,\n",
    "                data_dir=pollution_folder,\n",
    "                statpop_points_gdf=statpop_points_gdf,\n",
    "                statpop_ha_gdf=statpop_ha_gdf\n",
    "            )\n",
    "            all_stats.append(stats)\n",
    "        \n",
    "        print(\"All raster processing completed successfully.\")\n",
    "        \n",
    "        # Convert all_stats to a DataFrame\n",
    "        stats_df = pd.DataFrame(all_stats)\n",
    "        stats_df.index += 1  # Make index start from 1\n",
    "        \n",
    "        print(\"\\nStatistics summary for all processed rasters:\")\n",
    "        print(stats_df.describe())\n",
    "        \n",
    "        # Save the statistics\n",
    "        stats_output_file = \"raster_stats_summary.csv\"\n",
    "        stats_df.to_csv(os.path.join(pollution_folder, stats_output_file))\n",
    "        print(f\"\\nRaster statistics saved to {stats_output_file}\")\n",
    "        \n",
    "        # Save the updated GeoDataFrame\n",
    "        to_geofeather(statpop_ha_gdf.filter(regex='RELI|geometry|NO2_tropospheric'), output_path)\n",
    "        df_ha_no2 = from_geofeather(pollution_folder/'statpop_ha_with_NO2.feather')\n",
    "        print(f\"Updated GeoDataFrame saved to {output_file}\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "    stats_output_file = \"raster_stats_summary.csv\"\n",
    "    df_ha_no2 = from_geofeather(pollution_folder/'statpop_ha_with_NO2.feather')\n",
    "    stats_df = pd.read_csv(os.path.join(pollution_folder, stats_output_file))\n",
    "    print(f\"'{output_file}' already exists. Skipping processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_no2_tropospheric = pd.read_csv(pollution_folder/'statisticalDataOfNO2.csv')\n",
    "stats_no2_tropospheric  = stats_no2_tropospheric.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = stats_df.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondence_months_tropo = pd.merge(stats_df, stats_no2_tropospheric,how = 'left', right_on = 'tropospheric_NO2_column_number_density_max', left_on = 'max')\n",
    "correspondence_months_tropo.index +=1\n",
    "dict_months_tropo = correspondence_months_tropo['system:index'].to_dict() \n",
    "\n",
    "\n",
    "for key in dict_months_tropo.keys():\n",
    "    df_ha_no2 = df_ha_no2.rename(columns = {'NO2_tropospheric_{}'.format(key):'NO2_tropospheric_{}'.format(dict_months_tropo[key])})\n",
    "\n",
    "df_ha_no2['NO2_tropospheric-mean2019'] = df_ha_no2.filter(regex = 'NO2_tropospheric_2019').mean(axis = 1)\n",
    "df_ha_no2['NO2_tropospheric-mean2020'] = df_ha_no2.filter(regex = 'NO2_tropospheric_2020').mean(axis = 1)\n",
    "df_ha_no2['NO2_tropospheric-mean2021'] = df_ha_no2.filter(regex = 'NO2_tropospheric_2021').mean(axis = 1)\n",
    "df_ha_no2['NO2_tropospheric-mean2022'] = df_ha_no2.filter(regex = 'NO2_tropospheric_2022').mean(axis = 1)\n",
    "\n",
    "to_geofeather(df_ha_no2, pollution_folder/'df_ha_NO2_final.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no2_vd_month = df_ha_no2.filter(regex = 'NO2_tropospheric_|RELI').melt(id_vars = 'RELI')\n",
    "df_no2_vd_year = df_ha_no2.filter(regex = 'NO2_tropospheric-mean|RELI').melt(id_vars = 'RELI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no2_vd_month['month'] = pd.to_datetime(df_no2_vd_month['variable'].str[-7:].str.replace('_',''), format = '%Y%m')\n",
    "df_no2_vd_year['year'] = pd.to_datetime(df_no2_vd_year['variable'].str[-4:], format = '%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Merge with cluster persistence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_ha_no2.filter(regex = 'RELI|NO2'), on = 'RELI')\n",
    "df_period = pd.merge(df_period, df_ha_no2.filter(regex = 'RELI|NO2'), on = 'RELI')\n",
    "\n",
    "df_lausanne = pd.merge(df_lausanne, df_ha_no2.filter(regex = 'RELI|NO2'), on = 'RELI')\n",
    "df_period_lausanne = pd.merge(df_period_lausanne, df_ha_no2.filter(regex = 'RELI|NO2'), on = 'RELI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Final processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelling = df[['RELI','Persistence_survival','LST','NDVI','pm25','pm10','no2','noise_car_night','index_socio_stand','B20BTOT','B20BTOT_lag8','B20BTOT_lag24','B20BTOT_lag200m','E_KOORD','N_KOORD','testing_rate','NO2_tropospheric-mean2020','NO2_tropospheric-mean2021','NO2_tropospheric-mean2022','geometry']].reset_index(drop = True)\n",
    "df_modelling_lausanne = df_lausanne[['RELI','Persistence_survival','LST','NDVI','pm25','pm10','no2','noise_car_night','index_socio_stand','B20BTOT','B20BTOT_lag8','B20BTOT_lag24','B20BTOT_lag200m','E_KOORD','N_KOORD','testing_rate','NO2_tropospheric-mean2020','NO2_tropospheric-mean2021','NO2_tropospheric-mean2022','geometry']].reset_index(drop = True)\n",
    "\n",
    "df_modelling_period = df_period[['RELI','Persistence_survival','LST','NDVI','pm25','pm10','no2','noise_car_night','index_socio_stand','B20BTOT','B20BTOT_lag8','B20BTOT_lag24','B20BTOT_lag200m','E_KOORD','N_KOORD','testing_rate','NO2_tropospheric-mean2020','NO2_tropospheric-mean2021','NO2_tropospheric-mean2022','period','geometry']].reset_index(drop = True)\n",
    "df_modelling_period_lausanne = df_period_lausanne[['RELI','Persistence_survival','LST','NDVI','pm25','pm10','no2','noise_car_night','index_socio_stand','B20BTOT','B20BTOT_lag8','B20BTOT_lag24','B20BTOT_lag200m','E_KOORD','N_KOORD','testing_rate','NO2_tropospheric-mean2020','NO2_tropospheric-mean2021','NO2_tropospheric-mean2022','period','geometry']].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_col_names = {'B20BTOT':'Population',\n",
    "                 'B20BTOT_lag8':'Lagged population (8-NN)',\n",
    "                 'B20BTOT_lag24':'Lagged population (24-NN)',\n",
    "                 'B20BTOT_lag200m':'Lagged population (200m)',\n",
    "                 'testing_rate':'Testing rate (%)',\n",
    "                 'index_socio_stand':'SES index',\n",
    "                 'LST':'Land Surface Temperature',\n",
    "                 'NDVI':'NDVI',\n",
    "                 'pm10':'PM10',\n",
    "                'pm25':'PM25',\n",
    "                 'no2':'NO2',\n",
    "                 'noise_car_night':'Nighttime car noise',\n",
    "                 'NO2_tropospheric-mean2020':'Tropospheric NO2 (2020)',\n",
    "                 'NO2_tropospheric-mean2021':'Tropospheric NO2 (2021)',\n",
    "                 'NO2_tropospheric-mean2022':'Tropospheric NO2 (2022)',\n",
    "                 'E_KOORD':'E',\n",
    "                 'N_KOORD':'N',\n",
    "                 'period':'Period',\n",
    "                 'geometry':'geometry'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns\n",
    "df_modelling = df_modelling.rename(columns=dict_col_names)\n",
    "df_modelling_period = df_modelling_period.rename(columns=dict_col_names)\n",
    "\n",
    "df_modelling_lausanne = df_modelling_lausanne.rename(columns=dict_col_names)\n",
    "df_modelling_period_lausanne = df_modelling_period_lausanne.rename(columns=dict_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelling['Tropospheric NO2'] = df_modelling.filter(regex = \"Tropospheric\").mean(axis = 1)\n",
    "df_modelling_period['Tropospheric NO2'] = df_modelling_period.filter(regex = \"Tropospheric\").mean(axis = 1)\n",
    "\n",
    "df_modelling_lausanne['Tropospheric NO2'] = df_modelling_lausanne.filter(regex = \"Tropospheric\").mean(axis = 1)\n",
    "df_modelling_period_lausanne['Tropospheric NO2'] = df_modelling_period_lausanne.filter(regex = \"Tropospheric\").mean(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Modelling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hyperparameter domain to search over\n",
    "space = {\n",
    "        'max_depth':hp.choice('max_depth', np.arange(1, 30, 2, dtype=int)),\n",
    "        'colsample_bytree':hp.quniform('colsample_bytree', 0.3, 1.01, 0.1),\n",
    "        'min_child_weight':hp.choice('min_child_weight', np.arange(1, 30, 1, dtype=int)),\n",
    "        'subsample':        hp.quniform('subsample', 0.3, 1.01, 0.1),\n",
    "        'learning_rate':    hp.choice('learning_rate',    np.arange(0.1, 1.01, 0.1)),\n",
    "        'gamma': hp.quniform('gamma', 0.1, 5, 0.2),\n",
    "    \n",
    "        'objective':'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(df, X_eq, y_col, optimize, space, n_evals, file_prefix, include_spatial=True):\n",
    "    model_directory = results_folder / 'Modelling' / file_prefix\n",
    "    model_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    X_coords = df[X_eq + ['E', 'N']] if include_spatial else df[X_eq]\n",
    "    y = df[y_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_coords, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    d_train = xgboost.DMatrix(X_train, label=y_train)\n",
    "    d_test = xgboost.DMatrix(X_test, label=y_test)\n",
    "    d_all = xgboost.DMatrix(X_coords, label=y)\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    best_params_path = model_directory / 'best_params.pkl'\n",
    "    if not best_params_path.exists():\n",
    "        trials = Trials()\n",
    "        best_params = optimize(trials, space, n_evals=n_evals)\n",
    "        best_params = space_eval(space, best_params)\n",
    "        with open(model_directory / 'trials.pkl', 'wb') as f:\n",
    "            pickle.dump(trials, f)\n",
    "        with open(best_params_path, 'wb') as f:\n",
    "            pickle.dump(best_params, f)\n",
    "    else:\n",
    "        with open(best_params_path, 'rb') as f:\n",
    "            best_params = pickle.load(f)\n",
    "        with open(model_directory / 'trials.pkl', 'rb') as f:\n",
    "            trials = pickle.load(f)\n",
    "\n",
    "    # Train model\n",
    "    model_path = model_directory / 'model.pkl'\n",
    "    if not model_path.exists():\n",
    "        final_model = xgboost.train(\n",
    "            best_params, d_train, num_boost_round=500,\n",
    "            evals=[(d_train, \"train\"), (d_test, \"test\")],\n",
    "            verbose_eval=False, early_stopping_rounds=10\n",
    "        )\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(final_model, f)\n",
    "    else:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            final_model = pickle.load(f)\n",
    "\n",
    "    # Evaluate model on test set\n",
    "    y_pred_test = final_model.predict(d_test)\n",
    "    print(\"Test set performance:\")\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "    print(\"R2:\", r2_score(y_test, y_pred_test))\n",
    "\n",
    "    # Evaluate model on full dataset\n",
    "    y_pred_all = final_model.predict(d_all)\n",
    "    print(\"Full dataset performance:\")\n",
    "    print('RMSE:', np.sqrt(mean_squared_error(y, y_pred_all)))\n",
    "    print('R2: ', r2_score(y, y_pred_all))\n",
    "\n",
    "    interpretable_ml_shap_viz(df, final_model, X_coords, model_directory, include_spatial)\n",
    "    \n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(params, n_folds=5):\n",
    "    # If you want to sample the data !\n",
    "    # _X_coords_sample = X_coords.sample(frac = 0.2, random_state=333).sort_index()\n",
    "    # _y_sample = y[y.index.isin(_X_coords_sample.index)]\n",
    "    #Cross-validation\n",
    "    d_train = xgboost.DMatrix(X_coords, y)\n",
    "    \n",
    "    cv_results = xgboost.cv(params, d_train, nfold = n_folds, num_boost_round=500,\n",
    "                        early_stopping_rounds = 10, metrics = 'rmse', seed = 0)\n",
    "    \n",
    "    loss = min(cv_results['test-rmse-mean'])\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "def optimize(trials, space, n_evals):\n",
    "    \n",
    "    best = fmin(score, space, algo=tpe.suggest, max_evals=n_evals,trials=trials,\n",
    "                rstate=np.random.default_rng(333))#Add seed to fmin function\n",
    "    return best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretable_ml_shap_viz(df, model, X, model_directory: Union[str, Path], include_spatial: bool = True):\n",
    "    \"\"\"\n",
    "    Generate and save various SHAP (SHapley Additive exPlanations) visualizations for an XGBoost model.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing the data, including spatial information if include_spatial is True.\n",
    "        model: Trained XGBoost model.\n",
    "        X: Feature matrix used for training the model.\n",
    "        model_directory (Union[str, Path]): Directory to save the generated visualizations.\n",
    "        include_spatial (bool): Whether to include spatial visualizations. Defaults to True.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the model directory cannot be created.\n",
    "        Exception: For any other unexpected errors during execution.\n",
    "    \"\"\"\n",
    "    # try:\n",
    "    model_directory = Path(model_directory)\n",
    "    model_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    explainer_shap = shap.TreeExplainer(model)\n",
    "    shap_values = explainer_shap(X)\n",
    "    shap_interaction_values = explainer_shap.shap_interaction_values(X)\n",
    "\n",
    "    generate_shap_summary(shap_values, X, model_directory)\n",
    "    generate_shap_interaction_summary(shap_interaction_values, X, model_directory)\n",
    "    generate_dependence_plots(shap_values, X, model_directory)\n",
    "\n",
    "    if include_spatial:\n",
    "        generate_shap_maps(df, shap_values, X, model_directory)\n",
    "        generate_location_effect_map(df, shap_values, model_directory)\n",
    "\n",
    "    generate_variable_importance_plots(model, model_directory)\n",
    "    generate_force_plot(explainer_shap, shap_values, X, model_directory)\n",
    "\n",
    "    # except ValueError as ve:\n",
    "    #     print(f\"Error creating model directory: {ve}\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def generate_shap_summary(shap_values, X, model_directory):\n",
    "    shap.summary_plot(shap_values, feature_names=X.columns, show=False)\n",
    "    save_plot(model_directory, 'shap_summary')\n",
    "    plt.close()\n",
    "\n",
    "def generate_shap_interaction_summary(shap_interaction_values, X, model_directory):\n",
    "    shap.summary_plot(shap_interaction_values, X, max_display=16,\n",
    "                      feature_names=X.columns, show=False, plot_type=\"compact_dot\")\n",
    "    save_plot(model_directory, 'shap_summary_interaction_values')\n",
    "    plt.close()\n",
    "\n",
    "def generate_dependence_plots(shap_values, X, model_directory):\n",
    "    dependence_dir = model_directory / 'Dependence plots'\n",
    "    dependence_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for name in X.columns:\n",
    "        shap.dependence_plot(name, shap_values.values, X, display_features=X, alpha=0.4, interaction_index=None, show=False)\n",
    "        save_plot(dependence_dir, f'shap_dependence_{name}', formats=['png'])\n",
    "        plt.close()\n",
    "\n",
    "def generate_shap_maps(df, shap_values, X, model_directory):\n",
    "    n_columns = len(X.columns)\n",
    "    n_rows = int(np.ceil(n_columns / 3))\n",
    "    fig, ax = plt.subplots(n_rows, 3, figsize=(15, 15 * n_rows / 3))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for j in range(n_columns):\n",
    "        df.plot(ax=ax[j], column=shap_values.values[:, j], legend=True,\n",
    "                vmin=-0.8, vmax=0.8, cmap=shap.plots.colors.red_white_blue, legend_kwds={'shrink':0.5})\n",
    "        ax[j].set_title(\"SHAP for\\n\" + str(X.columns[j]), fontsize=10)\n",
    "        ax[j].set_axis_off()\n",
    "\n",
    "    for j in range(n_columns, n_rows * 3):\n",
    "        ax[j].set_axis_off()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_plot(model_directory, 'maps_shap_variables')\n",
    "    plt.close()\n",
    "def generate_location_effect_map(df, shap_values, model_directory):\n",
    "    fig, ax = plt.subplots(dpi=300)\n",
    "    df.plot(ax=ax, column=shap_values.values[:, -1] + shap_values.values[:, -2],\n",
    "            legend=True, vmin=-0.6, vmax=0.6, figsize=(15, 8),\n",
    "            cmap=shap.plots.colors.red_white_blue)\n",
    "    plt.title(\"Location Effect on Cluster persistence\\n(SHAP values of geographic coordinates)\\n\", fontsize=8)\n",
    "    plt.axis('off')\n",
    "    save_plot(model_directory, 'maps_shap_location')\n",
    "    plt.close()\n",
    "\n",
    "def generate_variable_importance_plots(model, model_directory):\n",
    "    xgboost.plot_importance(model)\n",
    "    plt.title(\"xgboost.plot_importance(model)\")\n",
    "    save_plot(model_directory, 'importance_plot_default')\n",
    "\n",
    "    xgboost.plot_importance(model, importance_type=\"cover\")\n",
    "    plt.title('xgboost.plot_importance(model, importance_type=\"cover\")')\n",
    "    save_plot(model_directory, 'importance_plot_cover')\n",
    "    plt.close()\n",
    "\n",
    "def generate_force_plot(explainer_shap, shap_values, X, model_directory):\n",
    "    shap.force_plot(explainer_shap.expected_value, shap_values.values[1, :], X.iloc[1, :], show=False, matplotlib=True)\n",
    "    save_plot(model_directory, 'force_plot')\n",
    "    plt.close()\n",
    "\n",
    "def save_plot(directory, filename, formats=['png']):\n",
    "    for fmt in formats:\n",
    "        plt.savefig(directory / f'{filename}.{fmt}', dpi=300 if fmt == 'png' else 180, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Run XGBoost + SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Whole canton\n",
    "#### Whole period\n",
    "##### Univariate analyses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Persistence_survival']\n",
    "X_eq0 = ['Population']\n",
    "X_eq1 = ['Lagged population (8-NN)']\n",
    "X_eq2 = ['Lagged population (200m)']\n",
    "X_eq3 = ['Lagged population (24-NN)']\n",
    "X_eq4 = ['Testing rate (%)']\n",
    "X_eq5 = ['SES index']\n",
    "X_eq6 = ['Land Surface Temperature']\n",
    "X_eq7 = ['NDVI']\n",
    "X_eq8 = ['PM10']\n",
    "X_eq9 = ['NO2']\n",
    "X_eq10 = ['Tropospheric NO2']\n",
    "X_eq11 = ['Nighttime car noise']\n",
    "X_eq12 = ['E']\n",
    "X_eq13 = ['N']\n",
    "X_eq14 = ['PM25']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = [X_eq0, X_eq1, X_eq2, X_eq3, X_eq4, X_eq5, X_eq6, X_eq7, X_eq8, X_eq9, X_eq10, X_eq11, X_eq12, X_eq13, X_eq14]\n",
    "\n",
    "for i, features in enumerate(feature_sets):\n",
    "    print(features)\n",
    "    train_xgboost(df_modelling, features, 'Persistence_survival', optimize, space, n_evals=500, file_prefix=f'Model {i} - Univariate', include_spatial=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "##### Multivariate analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_modelling['Persistence_survival']\n",
    "\n",
    "X_eq1 = ['Population','Lagged population (24-NN)','SES index','Testing rate (%)']\n",
    "X_eq2 = ['Population','Lagged population (24-NN)','SES index','Land Surface Temperature','NDVI','PM25','NO2','Nighttime car noise','Tropospheric NO2','Testing rate (%)']\n",
    "\n",
    "X_eq3 = ['Population','Lagged population (24-NN)','PM25','Testing rate (%)']\n",
    "X_eq4 = ['Population','Lagged population (24-NN)','PM10','Testing rate (%)']\n",
    "X_eq7 = ['Population','Lagged population (24-NN)','NO2','Testing rate (%)']\n",
    "X_eq8 = ['Population','Lagged population (24-NN)','Tropospheric NO2','Testing rate (%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Calculating VIF for each feature\n",
    "X_coords = df_modelling[X_eq3]\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = X_coords.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_coords.values, i) for i in range(X_coords.shape[1])]\n",
    "\n",
    "print(\"VIF values:\")\n",
    "print(vif_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_xgboost(df_modelling, X_eq1, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 1')\n",
    "# train_xgboost(df_modelling, X_eq2, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_xgboost(df_modelling, X_eq3, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 3')\n",
    "# train_xgboost(df_modelling, X_eq4, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 4')\n",
    "# train_xgboost(df_modelling, X_eq5, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 5')\n",
    "# train_xgboost(df_modelling, X_eq6, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### Period-wise analysis\n",
    "##### Multivariate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['RELI', 'Land Surface Temperature', 'NDVI', 'SES index', 'PM10','PM25', 'NO2', 'Nighttime car noise', 'Population', 'Lagged population (8-NN)', 'Lagged population (24-NN)','Lagged population (200m)', 'Tropospheric NO2 (2020)', 'Tropospheric NO2 (2021)', 'Tropospheric NO2 (2022)', 'Testing rate (%)']\n",
    "periods = ['first_period', 'second_period', 'third_period', 'fourth_period', 'fifth_period']\n",
    "dfs = []\n",
    "for period in periods:\n",
    "    df_period = df_modelling_period[df_modelling_period['Period'] == period]\n",
    "    df_period.loc[df_period['NO2'] > df_period['NO2'].quantile(0.999), 'NO2'] = df_period['NO2'].mean()\n",
    "    df_period.loc[df_period['PM10'] > df_period['PM10'].quantile(0.999), 'PM10'] = df_period['PM10'].mean()\n",
    "    df_period = pd.merge(df_period, df_ha_no2.filter(regex='RELI|NO2'), on='RELI')\n",
    "    df_period = gpd.GeoDataFrame(df_period, geometry=df_period['geometry'], crs=2056)\n",
    "\n",
    "    dfs.append(df_period)\n",
    "\n",
    "df_period_1, df_period_2, df_period_3, df_period_4, df_period_5 = dfs\n",
    "df_period_1['Tropospheric NO2 (periodic avg)'] = df_period_1[['NO2_tropospheric_2020_3','NO2_tropospheric_2020_4','NO2_tropospheric_2020_5','NO2_tropospheric_2020_6']].mean( axis = 1)\n",
    "df_period_2['Tropospheric NO2 (periodic avg)'] = df_period_2[['NO2_tropospheric_2020_7','NO2_tropospheric_2020_8','NO2_tropospheric_2020_9','NO2_tropospheric_2020_10','NO2_tropospheric_2020_11','NO2_tropospheric_2020_12']].mean( axis = 1)\n",
    "df_period_3['Tropospheric NO2 (periodic avg)'] = df_period_3[['NO2_tropospheric_2021_1','NO2_tropospheric_2021_2','NO2_tropospheric_2021_3','NO2_tropospheric_2021_4','NO2_tropospheric_2021_5']].mean( axis = 1)\n",
    "df_period_4['Tropospheric NO2 (periodic avg)'] = df_period_4[['NO2_tropospheric_2021_6','NO2_tropospheric_2021_7','NO2_tropospheric_2021_8','NO2_tropospheric_2021_9','NO2_tropospheric_2021_10', 'NO2_tropospheric_2021_11']].mean( axis = 1)\n",
    "df_period_5['Tropospheric NO2 (periodic avg)'] = df_period_5[['NO2_tropospheric_2021_12','NO2_tropospheric_2022_1','NO2_tropospheric_2022_2','NO2_tropospheric_2022_3','NO2_tropospheric_2022_4']].mean( axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "##### Dimensionality reduction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def apply_pca_to_df(df, feature_names, new_feature_name, inverted=False):\n",
    "    # Selecting the relevant features\n",
    "    X = df[feature_names]\n",
    "\n",
    "    # Standardizing the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Applying PCA\n",
    "    pca = PCA(n_components=1)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # # Scaling PCA results to range 0-100\n",
    "    # scaler_100 = MinMaxScaler(feature_range=(0, 100))\n",
    "    # X_pca_scaled = scaler_100.fit_transform(X_pca)\n",
    "\n",
    "    df[new_feature_name] = X_pca[:, 0]\n",
    "    if inverted:\n",
    "        # Inverting the scale\n",
    "        X_pca_inverted = 100 - X_pca\n",
    "        # Adding the inverted scaled PCA result back to the dataframe\n",
    "        df[new_feature_name] = X_pca_inverted[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to combine into a single PCA component\n",
    "features_to_combine = ['Land Surface Temperature', 'NDVI', 'Nighttime car noise']\n",
    "new_feature_name = 'Urban type index'\n",
    "\n",
    "# Applying PCA to each DataFrame\n",
    "dataframes = [df_period_1, df_period_2, df_period_3, df_period_4, df_period_5]\n",
    "for i, df in enumerate(dataframes, start=1):\n",
    "    dataframes[i-1] = apply_pca_to_df(df, features_to_combine, new_feature_name, inverted=False)\n",
    "    print(f\"PCA applied to gwr_df_vd_period_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features to combine into a single PCA component\n",
    "features_to_combine = ['NO2','PM25','PM10']\n",
    "new_feature_name = 'Air pollution index'\n",
    "\n",
    "# Applying PCA to each DataFrame\n",
    "dataframes = [df_period_1, df_period_2, df_period_3, df_period_4, df_period_5]\n",
    "for i, df in enumerate(dataframes, start=1):\n",
    "    dataframes[i-1] = apply_pca_to_df(df, features_to_combine, new_feature_name)\n",
    "    print(f\"PCA applied to gwr_df_vd_period_{i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "##### Multicollinearity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Assuming df_period_1, df_period_2, ..., df_period_5 are already defined\n",
    "\n",
    "periods = ['first', 'second', 'third', 'fourth', 'fifth']\n",
    "dfs = [df_period_1, df_period_2, df_period_3, df_period_4, df_period_5]\n",
    "\n",
    "features = ['Population', 'Lagged population (24-NN)', 'Air pollution index', \n",
    "            'Tropospheric NO2 (periodic avg)', 'Testing rate (%)', 'Urban type index']\n",
    "\n",
    "for period, df in zip(periods, dfs):\n",
    "    print(f\"\\nCalculating VIF for {period} period:\")\n",
    "    \n",
    "    # Selecting features for VIF calculation\n",
    "    X_coords = df[features]\n",
    "    \n",
    "    # Calculating VIF for each feature\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X_coords.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X_coords.values, i) \n",
    "                       for i in range(X_coords.shape[1])]\n",
    "    \n",
    "    # Sorting VIF values in descending order\n",
    "    vif_data = vif_data.sort_values('VIF', ascending=False)\n",
    "    \n",
    "    print(vif_data)\n",
    "    \n",
    "    # Optional: You can add a threshold check here\n",
    "    high_vif = vif_data[vif_data['VIF'] > 5]  # Assuming 5 as a threshold\n",
    "    if not high_vif.empty:\n",
    "        print(f\"\\nFeatures with high VIF (>5) in {period} period:\")\n",
    "        print(high_vif)\n",
    "    else:\n",
    "        print(f\"\\nNo features with high VIF (>5) in {period} period.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "##### Run models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eq1_wc = ['Population','Lagged population (24-NN)','Air pollution index','Tropospheric NO2 (periodic avg)','Testing rate (%)', 'Urban type index']\n",
    "X_eq2_wc = ['Population','Lagged population (24-NN)','SES index','Testing rate (%)', 'Urban type index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [df_period_1, df_period_2, df_period_3, df_period_4, df_period_5]\n",
    "\n",
    "for i, period in enumerate(periods, start=1):\n",
    "    y = period['Persistence_survival']\n",
    "    train_xgboost(period, X_eq1_wc, 'Persistence_survival', optimize, space, n_evals=500, file_prefix=f'Model 1 - Period {i}')\n",
    "    train_xgboost(period, X_eq2_wc, 'Persistence_survival', optimize, space, n_evals=500, file_prefix=f'Model 2 - Period {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Lausanne urban area\n",
    "#### Whole period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelling_lausanne[['RELI','NO2']][df_modelling_lausanne['RELI'] == 53821550]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eq1 = ['Population','Lagged population (24-NN)','SES index','Testing rate (%)']\n",
    "X_eq2 = ['Population','Lagged population (24-NN)','SES index','Land Surface Temperature','NDVI','PM25','NO2','Nighttime car noise','Tropospheric NO2','Testing rate (%)']\n",
    "\n",
    "X_eq3 = ['Population','Lagged population (24-NN)','PM25','Testing rate (%)']\n",
    "X_eq4 = ['Population','Lagged population (24-NN)','PM10','Testing rate (%)']\n",
    "X_eq5 = ['Population','Lagged population (24-NN)','NO2','Testing rate (%)']\n",
    "X_eq6 = ['Population','Lagged population (24-NN)','Tropospheric NO2','Testing rate (%)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgboost(df_modelling_lausanne, X_eq1, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 1 - Lausanne urban area')\n",
    "train_xgboost(df_modelling_lausanne, X_eq2, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 2 - Lausanne urban area')\n",
    "train_xgboost(df_modelling_lausanne, X_eq3, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 3 - Lausanne urban area')\n",
    "train_xgboost(df_modelling_lausanne, X_eq4, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 4 - Lausanne urban area')\n",
    "train_xgboost(df_modelling_lausanne, X_eq5, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 5 - Lausanne urban area')\n",
    "train_xgboost(df_modelling_lausanne, X_eq6, 'Persistence_survival', optimize, space, n_evals=500, file_prefix='Model 6 - Lausanne urban area')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
