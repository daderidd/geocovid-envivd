{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from geofeather import to_geofeather, from_geofeather\n",
    "from pathlib import Path\n",
    "from shapely import Polygon\n",
    "import datetime\n",
    "import libpysal as lps\n",
    "import matplotlib.pyplot as plt\n",
    "from libpysal.weights.distance import get_points_array\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "sys.path.append('./utils')\n",
    "import utils\n",
    "data_folder = Path('../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Load testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load toy COVID data\n",
    "df_covid = pd.read_csv(data_folder/'covid_data_example.csv')\n",
    "df_covid = gpd.GeoDataFrame(df_covid, crs=4326, geometry=gpd.points_from_xy(df_covid.longitude, df_covid.latitude))\n",
    "df_covid = df_covid.to_crs(2056)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Load geographic elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Populated hectares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## VD statpop\n",
    "statpop_ha_file = \"statpop_communes_vd_ha.feather\"\n",
    "statpop_ha_gdf = from_geofeather(data_folder/'ag-b-00.03-vz2020statpop'/statpop_ha_file)\n",
    "statpop_ha_gdf_light = statpop_ha_gdf[['RELI','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entire CH statpop\n",
    "ch_statpop_ha = pd.read_csv(data_folder/'ag-b-00.03-vz2020statpop/STATPOP2020.csv',sep = ';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "communes = gpd.read_file(data_folder/'Administrative units'/'swissBOUNDARIES3D_1_3_TLM_HOHEITSGEBIET.shp', engine='pyogrio')\n",
    "communes_vd = communes[communes.KANTONSNUM == 22]\n",
    "communes_vd = communes_vd[~communes_vd.geometry.isnull()]\n",
    "communes_vd = communes_vd.rename(columns={'geom': 'geometry'})\n",
    "communes_vd = communes_vd[communes_vd.NAME != 'Lac Léman (VD)']\n",
    "communes_vd = communes_vd[communes_vd.NAME != 'Lac de Neuchâtel (VD)']\n",
    "communes_vd = communes_vd[communes_vd.NAME != 'Lac de Morat (VD)']\n",
    "communes_vd = communes_vd.reset_index(drop=True)\n",
    "communes_vd.crs = 2056\n",
    "communes_vd = gpd.GeoDataFrame(communes_vd, crs = 2056,geometry=communes_vd['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lausanne \n",
    "xmin, ymin, xmax, ymax = communes_vd[communes_vd.NAME.isin(['Lausanne','Renens (VD)','Prilly','Ecublens (VD)','Pully','Le Mont-sur-Lausanne','Paudex','Lutry'])].total_bounds\n",
    "gdf_lausanne = statpop_ha_gdf.cx[xmin:xmax, ymin:ymax]\n",
    "bbox_geometry = [Polygon([(xmin,ymin),(xmax,ymin),(xmax,ymax),(xmin, ymax)])]\n",
    "bbox_lausanne = gpd.GeoDataFrame(['Lausanne'], crs = 2056, geometry = bbox_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Define study periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time periods for analysis\n",
    "p1_start = '2020-01-10'\n",
    "p2_start = '2020-06-30'\n",
    "p3_start = '2020-12-16'\n",
    "p4_start = '2021-05-07'\n",
    "p5_start = '2021-11-28'\n",
    "p5_end = '2022-04-16'\n",
    "\n",
    "df_covid.loc[df_covid.date_reception.between(p1_start, p2_start), 'period'] = 'p1'\n",
    "df_covid.loc[df_covid.date_reception.between(p2_start, p3_start), 'period'] = 'p2'\n",
    "df_covid.loc[df_covid.date_reception.between(p3_start, p4_start), 'period'] = 'p3'\n",
    "df_covid.loc[df_covid.date_reception.between(p4_start, p5_start), 'period'] = 'p4'\n",
    "df_covid.loc[df_covid.date_reception.between(p5_start, p5_end), 'period'] = 'p5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Select cases by period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid['date_reception'] = pd.to_datetime(df_covid['date_reception']).dt.strftime('%Y-%m-%d')\n",
    "df_covid['month'] = pd.to_datetime(df_covid['date_reception']).dt.strftime('%Y-%m')\n",
    "df_covid['week'] = pd.to_datetime(df_covid['date_reception']).dt.isocalendar().week\n",
    "df_covid['year'] = pd.to_datetime(df_covid['date_reception']).dt.isocalendar().year\n",
    "df_covid['week_str'] = df_covid.apply(\n",
    "    lambda row: datetime.datetime.strptime(f\"{row['year']}-{row['week']}-1\", \"%Y-%W-%w\"),\n",
    "    axis=1\n",
    ")\n",
    "df_covid_in_ha = gpd.sjoin(statpop_ha_gdf[['RELI','B20BTOT','NAME','EINWOHNERZ','geometry']], df_covid[['id_demande_study2','date_reception','res_cov_txt','week','week_str','month','year','period','geometry']], predicate = 'intersects',how = 'right').sort_values('B20BTOT',ascending = False) #We sort to make sure that the first hectare is the most populated\n",
    "df_covid_in_ha = df_covid_in_ha.drop_duplicates(subset = ['id_demande_study2'],keep = 'first') #We keep only the first duplicate (most populated)\n",
    "df_covid_in_ha = df_covid_in_ha.drop('index_left',axis = 1)\n",
    "df_covid_in_ha['RELI'] = df_covid_in_ha.apply(lambda row: utils.apply_min_dist(row, statpop_ha_gdf), axis=1)\n",
    "df_covid_in_ha['period'] = df_covid_in_ha['period'].str[1].fillna('0').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = df_covid_in_ha[df_covid_in_ha.res_cov_txt == 'POSITIVE']\n",
    "\n",
    "cases_first_period = cases[(cases.date_reception >= p1_start) & (cases.date_reception < p2_start)]\n",
    "cases_second_period = cases[(cases.date_reception >= p2_start) & (cases.date_reception < p3_start)]\n",
    "cases_third_period = cases[(cases.date_reception >= p3_start) & (cases.date_reception < p4_start)]\n",
    "cases_fourth_period = cases[(cases.date_reception >= p4_start) & (cases.date_reception < p5_start)]\n",
    "cases_fifth_period = cases[(cases.date_reception >= p5_start) & (cases.date_reception < p5_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_lausanne = cases[cases.RELI.isin(gdf_lausanne.RELI)]\n",
    "\n",
    "cases_first_period_lausanne = cases_first_period[cases_first_period.RELI.isin(gdf_lausanne.RELI)]\n",
    "cases_second_period_lausanne = cases_second_period[cases_second_period.RELI.isin(gdf_lausanne.RELI)]\n",
    "cases_third_period_lausanne = cases_third_period[cases_third_period.RELI.isin(gdf_lausanne.RELI)]\n",
    "cases_fourth_period_lausanne = cases_fourth_period[cases_fourth_period.RELI.isin(gdf_lausanne.RELI)]\n",
    "cases_fifth_period_lausanne = cases_fifth_period[cases_fifth_period.RELI.isin(gdf_lausanne.RELI)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Testing rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covid_in_ha_fullperiod = df_covid_in_ha.copy()\n",
    "df_covid_in_ha_fullperiod['period'] = 'full'\n",
    "df_covid_in_ha_w_fullperiod = pd.concat([df_covid_in_ha, df_covid_in_ha_fullperiod])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_rates = df_covid_in_ha_w_fullperiod.groupby(['RELI','period']).agg({'id_demande_study2':'size','B20BTOT':'first'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_rates.columns = ['n_tests','population']\n",
    "df_testing_rates = df_testing_rates.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_rates['testing_rate'] = (df_testing_rates['n_tests']/df_testing_rates['population'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing_rates.loc[df_testing_rates['testing_rate'] > 100, 'testing_rate'] = 100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Functions for MST-DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_multiple_survival_times(x: pd.Series, first_date: str, last_date: str) -> Tuple[List[int], List[int], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Extract multiple survival times from a series of cluster statuses.\n",
    "\n",
    "    Args:\n",
    "        x (pd.Series): Series containing cluster statuses.\n",
    "        first_date (str): Start date of the analysis period.\n",
    "        last_date (str): End date of the analysis period.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[int], List[int], List[int], List[int]]: Survival times, time between clusters, status, and status_resistance.\n",
    "    \"\"\"\n",
    "    cluster_statuses = [status.replace('keep', 'cluster').replace('increase', 'cluster').replace('decrease', 'cluster') \n",
    "                        for status in x]\n",
    "    \n",
    "    grouped_statuses = [(status, sum(1 for _ in group)) \n",
    "                        for status, group in itertools.groupby(cluster_statuses)]\n",
    "    \n",
    "    first_date_ts = pd.Timestamp(first_date)\n",
    "    last_date_ts = pd.Timestamp(last_date)\n",
    "    total_days = (last_date_ts - first_date_ts).days + 1\n",
    "\n",
    "    survival_times, time_between_clusters, status, status_resistance = [], [], [], []\n",
    "    cumulative_days = 0\n",
    "\n",
    "    for status_type, duration in grouped_statuses:\n",
    "        cumulative_days += duration\n",
    "        if status_type == 'cluster':\n",
    "            survival_times.append(duration)\n",
    "            status.append(0 if cumulative_days == total_days else 1)\n",
    "        else:\n",
    "            time_between_clusters.append(duration)\n",
    "            status_resistance.append(0 if cumulative_days == total_days else 1)\n",
    "\n",
    "    return survival_times, time_between_clusters, status, status_resistance\n",
    "\n",
    "def prep_survival(polygons: pd.DataFrame, first_date: str, last_date: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Prepare survival analysis data from polygon data.\n",
    "\n",
    "    Args:\n",
    "        polygons (pd.DataFrame): DataFrame containing polygon data.\n",
    "        first_date (str): Start date of the analysis period.\n",
    "        last_date (str): End date of the analysis period.\n",
    "        dict_ses_class (Dict): Dictionary mapping RELI to SES class.\n",
    "        dict_ses_class_q3 (Dict): Dictionary mapping RELI to SES class (quartile 3).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: Survival data, resistance data, and multiple survival times data.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "\n",
    "    multiple_survivals = polygons.set_index('RELI').loc[:, first_date:last_date].apply(\n",
    "        lambda x: extract_multiple_survival_times(x, first_date, last_date), axis=1\n",
    "    ).to_dict()\n",
    "    multiple_survivals = pd.DataFrame.from_dict(multiple_survivals).T\n",
    "\n",
    "    multiple_survivals.columns = ['Survival times', 'Time between clusters', 'status', 'status_resistance']\n",
    "\n",
    "    df_mult_survival_times = multiple_survivals['Survival times'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()\n",
    "    df_mult_survival_times.columns = ['RELI', 'cluster_n', 'Persistence_survival']\n",
    "\n",
    "    df_time_bt_clusters = multiple_survivals['Time between clusters'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()\n",
    "    df_time_bt_clusters.columns = ['RELI', 'interval_n', 'Resistance_survival']\n",
    "\n",
    "    df_status = multiple_survivals['status'].apply(pd.Series).reset_index().melt(id_vars='index').dropna().reset_index(drop=True)\n",
    "    df_status.columns = ['RELI', 'cluster_n', 'status']\n",
    "\n",
    "    df_status_resistance = multiple_survivals['status_resistance'].apply(pd.Series).reset_index().melt(id_vars='index').dropna().reset_index(drop=True)\n",
    "    df_status_resistance.columns = ['RELI', 'interval_n', 'status_resistance']\n",
    "\n",
    "    df_survival = pd.merge(df_mult_survival_times, df_status, on=['RELI', 'cluster_n'])\n",
    "    df_survival['status'] = df_survival['status'].map({1: True, 0: False})\n",
    "    df_survival['Survival_scaled'] = scaler.fit_transform(df_survival[['Persistence_survival']])\n",
    "\n",
    "    df_survival_resistance = pd.merge(df_time_bt_clusters, df_status_resistance, on=['RELI', 'interval_n'])\n",
    "    df_survival_resistance['status_resistance'] = df_survival_resistance['status_resistance'].map({1: True, 0: False})\n",
    "    df_survival_resistance['Survival_scaled'] = scaler.fit_transform(df_survival_resistance[['Resistance_survival']])\n",
    "    return df_survival, df_survival_resistance, df_mult_survival_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysda\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "def analyze_period(\n",
    "    cases_data: pd.DataFrame,\n",
    "    period_name: str,\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    population_data: Any,\n",
    "    res_folder: Path,\n",
    "    eps_spatial: float = 200,\n",
    "    eps_temporal_low: float = 1,\n",
    "    eps_temporal_high: float = 14,\n",
    "    min_pts: int = 3,\n",
    "    moving_ratio: float = 0.1,\n",
    "    area_ratio: float = 0.1\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze a specific period of data using MSTDBSCAN and prepare survival analysis.\n",
    "\n",
    "    Args:\n",
    "        cases_data (pd.DataFrame): DataFrame containing case data for the period.\n",
    "        period_name (str): Name of the period (e.g., '1st period', '2nd period').\n",
    "        start_date (str): Start date of the period in format 'YYYY/MM/DD-HH:MM:SS'.\n",
    "        end_date (str): End date of the period in format 'YYYY/MM/DD-HH:MM:SS'.\n",
    "        gdf_soc_final (Any): GeoDataFrame for setting polygons.\n",
    "        res_folder (Path): Path to the results folder.\n",
    "        eps_spatial (float): Spatial epsilon parameter for MSTDBSCAN.\n",
    "        eps_temporal_low (float): Lower temporal epsilon parameter for MSTDBSCAN.\n",
    "        eps_temporal_high (float): Higher temporal epsilon parameter for MSTDBSCAN.\n",
    "        min_pts (int): Minimum points parameter for MSTDBSCAN.\n",
    "        moving_ratio (float): Moving ratio parameter for MSTDBSCAN.\n",
    "        area_ratio (float): Area ratio parameter for MSTDBSCAN.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Dictionary containing the results of the analysis.\n",
    "    \"\"\"\n",
    "    # Read data\n",
    "    pysda_data = pysda.data.readGDF(cases_data, timeColumn='date_reception', timeUnit=\"day\")\n",
    "\n",
    "    # Set up and run MSTDBSCAN\n",
    "    mst = pysda.MSTDBSCAN(pysda_data)\n",
    "    mst.setParams(eps_spatial, eps_temporal_low, eps_temporal_high, min_pts, moving_ratio, area_ratio)\n",
    "    mst.run()\n",
    "\n",
    "    # Process results\n",
    "    result = mst.result\n",
    "    result.setPolygons(population_data)\n",
    "    all_results = result.getAll()\n",
    "\n",
    "    clusters = all_results[\"clusters\"]\n",
    "    polygons = all_results[\"polygons\"]\n",
    "    points = all_results[\"points\"]\n",
    "\n",
    "    # Prepare survival analysis\n",
    "    survival, time_bt_clusters, mult_survival = prep_survival(polygons, start_date, end_date)\n",
    "\n",
    "    # Add period information\n",
    "    survival['period'] = period_name\n",
    "    time_bt_clusters['period'] = period_name\n",
    "    clusters['period'] = period_name\n",
    "\n",
    "    # Save results\n",
    "    survival_folder = res_folder / 'Survival analyses'\n",
    "    survival_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    survival.to_pickle(survival_folder / f'_{period_name.replace(\" \", \"_\")}_survival.pkl')\n",
    "    time_bt_clusters.to_pickle(survival_folder / f'_{period_name.replace(\" \", \"_\")}_time_bt.pkl')\n",
    "\n",
    "    return {\n",
    "        'clusters': clusters,\n",
    "        'polygons': polygons,\n",
    "        'points': points,\n",
    "        'survival': survival,\n",
    "        'time_bt_clusters': time_bt_clusters\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## MSTDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mstdbscan_folder = data_folder/'MSTDBSCAN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Canton of Vaud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_periods(periods: List[Dict[str, Any]], population_data: pd.DataFrame, output_folder: str) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Analyze all periods and return the results.\n",
    "\n",
    "    Args:\n",
    "        periods (List[Dict[str, Any]]): List of dictionaries containing period information.\n",
    "        population_data (pd.DataFrame): The population data for analysis.\n",
    "        output_folder (str): The folder path for saving output files.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, Any]]: A dictionary containing results for all periods.\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "\n",
    "    for period in periods:\n",
    "        period_results = analyze_period(\n",
    "            cases_data=period['cases_data'],\n",
    "            period_name=period['name'],\n",
    "            start_date=period['start_date'],\n",
    "            end_date=period['end_date'],\n",
    "            population_data=population_data,\n",
    "            res_folder=output_folder\n",
    "        )\n",
    "        \n",
    "        all_results[period['name']] = {\n",
    "            'clusters': period_results['clusters'],\n",
    "            'survival_data': period_results['survival'],\n",
    "            'intercluster_times': period_results['time_bt_clusters']\n",
    "        }\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "\n",
    "# Define period information\n",
    "analysis_periods = [\n",
    "    {\n",
    "        'name': 'first_period',\n",
    "        'cases_data': cases_first_period,\n",
    "        'start_date': '2020/03/02-00:00:00',\n",
    "        'end_date': '2020/06/29-00:00:00'\n",
    "    },\n",
    "    {\n",
    "        'name': 'second_period',\n",
    "        'cases_data': cases_second_period,\n",
    "        'start_date': '2020/06/30-00:00:00',\n",
    "        'end_date': '2020/12/15-00:00:00'\n",
    "    },\n",
    "    {\n",
    "        'name': 'third_period',\n",
    "        'cases_data': cases_third_period,\n",
    "        'start_date': '2020/12/16-00:00:00',\n",
    "        'end_date': '2021/05/06-00:00:00'\n",
    "    },\n",
    "    {\n",
    "        'name': 'fourth_period',\n",
    "        'cases_data': cases_fourth_period,\n",
    "        'start_date': '2021/05/07-00:00:00',\n",
    "        'end_date': '2021/11/27-00:00:00'\n",
    "    },\n",
    "    {\n",
    "        'name': 'fifth_period',\n",
    "        'cases_data': cases_fifth_period,\n",
    "        'start_date': '2021/11/28-00:00:00',\n",
    "        'end_date': '2022/04/15-00:00:00'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all periods\n",
    "all_period_results = analyze_all_periods(analysis_periods, population_data=statpop_ha_gdf_light, output_folder=mstdbscan_folder)\n",
    "\n",
    "# Access results\n",
    "clusters_first_period = all_period_results['first_period']['clusters']\n",
    "survival_first_period = all_period_results['first_period']['survival_data']\n",
    "intercluster_times_first_period = all_period_results['first_period']['intercluster_times']\n",
    "\n",
    "clusters_second_period = all_period_results['second_period']['clusters']\n",
    "survival_second_period = all_period_results['second_period']['survival_data']\n",
    "intercluster_times_second_period = all_period_results['second_period']['intercluster_times']\n",
    "\n",
    "clusters_third_period = all_period_results['third_period']['clusters']\n",
    "survival_third_period = all_period_results['third_period']['survival_data']\n",
    "intercluster_times_third_period = all_period_results['third_period']['intercluster_times']\n",
    "\n",
    "clusters_fourth_period = all_period_results['fourth_period']['clusters']\n",
    "survival_fourth_period = all_period_results['fourth_period']['survival_data']\n",
    "intercluster_times_fourth_period = all_period_results['fourth_period']['intercluster_times']\n",
    "\n",
    "clusters_fifth_period = all_period_results['fifth_period']['clusters']\n",
    "survival_fifth_period = all_period_results['fifth_period']['survival_data']\n",
    "intercluster_times_fifth_period = all_period_results['fifth_period']['intercluster_times']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Lausanne urban area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define period information\n",
    "analysis_periods_lausanne = [\n",
    "    {\n",
    "        'name': 'first_period',\n",
    "        'cases_data': cases_first_period_lausanne,\n",
    "        'start_date': '2020/03/07-00:00:00',\n",
    "        'end_date': '2020/06/29-00:00:00'\n",
    "    },\n",
    "    {\n",
    "        'name': 'second_period',\n",
    "        'cases_data': cases_second_period_lausanne,\n",
    "        'start_date': '2020/06/30-00:00:00',\n",
    "        'end_date': '2020/12/15-00:00:00'\n",
    "    },\n",
    "    {\n",
    "        'name': 'third_period',\n",
    "        'cases_data': cases_third_period_lausanne,\n",
    "        'start_date': '2020/12/16-00:00:00',\n",
    "        'end_date': '2021/05/06-00:00:00'\n",
    "    },\n",
    "    {\n",
    "        'name': 'fourth_period',\n",
    "        'cases_data': cases_fourth_period_lausanne,\n",
    "        'start_date': '2021/05/07-00:00:00',\n",
    "        'end_date': '2021/11/27-00:00:00'\n",
    "    },\n",
    "    {\n",
    "        'name': 'fifth_period',\n",
    "        'cases_data': cases_fifth_period_lausanne,\n",
    "        'start_date': '2021/11/28-00:00:00',\n",
    "        'end_date': '2022/04/15-00:00:00'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all periods\n",
    "all_period_results_lausanne = analyze_all_periods(analysis_periods_lausanne, population_data=statpop_ha_gdf_light, output_folder=mstdbscan_folder)\n",
    "\n",
    "# Access results\n",
    "clusters_first_period_lausanne = all_period_results_lausanne['first_period']['clusters']\n",
    "survival_first_period_lausanne = all_period_results_lausanne['first_period']['survival_data']\n",
    "intercluster_times_first_period_lausanne = all_period_results_lausanne['first_period']['intercluster_times']\n",
    "\n",
    "clusters_second_period_lausanne = all_period_results_lausanne['second_period']['clusters']\n",
    "survival_second_period_lausanne = all_period_results_lausanne['second_period']['survival_data']\n",
    "intercluster_times_second_period_lausanne = all_period_results_lausanne['second_period']['intercluster_times']\n",
    "\n",
    "clusters_third_period_lausanne = all_period_results_lausanne['third_period']['clusters']\n",
    "survival_third_period_lausanne = all_period_results_lausanne['third_period']['survival_data']\n",
    "intercluster_times_third_period_lausanne = all_period_results_lausanne['third_period']['intercluster_times']\n",
    "\n",
    "clusters_fourth_period_lausanne = all_period_results_lausanne['fourth_period']['clusters']\n",
    "survival_fourth_period_lausanne = all_period_results_lausanne['fourth_period']['survival_data']\n",
    "intercluster_times_fourth_period_lausanne = all_period_results_lausanne['fourth_period']['intercluster_times']\n",
    "\n",
    "clusters_fifth_period_lausanne = all_period_results_lausanne['fifth_period']['clusters']\n",
    "survival_fifth_period_lausanne = all_period_results_lausanne['fifth_period']['survival_data']\n",
    "intercluster_times_fifth_period_lausanne = all_period_results_lausanne['fifth_period']['intercluster_times']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Combine the five periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_combined_data(dataframes: List[pd.DataFrame], filename: str, res_folder: Path, file_format: str = 'parquet'):\n",
    "    \"\"\"\n",
    "    Combine multiple dataframes and save the result in the specified format.\n",
    "\n",
    "    Args:\n",
    "        dataframes (List[pd.DataFrame]): List of dataframes to combine.\n",
    "        filename (str): Name of the output file (without extension).\n",
    "        res_folder (Path): Path to the results folder.\n",
    "        file_format (str): Format to save the file in ('parquet', 'csv', or 'pickle'). Defaults to 'parquet'.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unsupported file format is specified.\n",
    "    \"\"\"\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    output_path = res_folder / f'{filename}.{file_format}'\n",
    "\n",
    "    try:\n",
    "        if file_format == 'parquet':\n",
    "            combined_df.to_parquet(output_path, index=False)\n",
    "        elif file_format == 'csv':\n",
    "            combined_df.to_csv(output_path, index=False)\n",
    "        elif file_format == 'pickle':\n",
    "            combined_df.to_pickle(output_path, protocol=4)  # Using protocol 4 for compatibility\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_format}\")\n",
    "        \n",
    "        print(f\"Successfully saved {filename}.{file_format}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {filename}.{file_format}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_combined_data([clusters_first_period, clusters_second_period, clusters_third_period, clusters_fourth_period, clusters_fifth_period], \n",
    "                   'combined_clusters', mstdbscan_folder)\n",
    "\n",
    "save_combined_data([survival_first_period, survival_second_period, survival_third_period, survival_fourth_period, survival_fifth_period], \n",
    "                   'combined_survival', mstdbscan_folder)\n",
    "\n",
    "save_combined_data([intercluster_times_first_period, intercluster_times_second_period, intercluster_times_third_period, intercluster_times_fourth_period, intercluster_times_fifth_period], \n",
    "                   'combined_time_bt', mstdbscan_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_combined_data([clusters_first_period_lausanne, clusters_second_period_lausanne, clusters_third_period_lausanne, clusters_fourth_period_lausanne, clusters_fifth_period_lausanne], \n",
    "                   'combined_clusters_lausanne', mstdbscan_folder)\n",
    "\n",
    "save_combined_data([survival_first_period_lausanne, survival_second_period_lausanne, survival_third_period_lausanne, survival_fourth_period_lausanne, survival_fifth_period_lausanne], \n",
    "                   'combined_survival_lausanne', mstdbscan_folder)\n",
    "\n",
    "save_combined_data([intercluster_times_first_period_lausanne, intercluster_times_second_period_lausanne, intercluster_times_third_period_lausanne, intercluster_times_fourth_period_lausanne, intercluster_times_fifth_period_lausanne], \n",
    "                   'combined_time_bt_lausanne', mstdbscan_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Clusters over the whole period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_whole_period = analyze_period(\n",
    "    cases,\n",
    "    'Whole period',\n",
    "    '2020/03/02-00:00:00',\n",
    "    '2022/04/15-00:00:00',\n",
    "    statpop_ha_gdf,\n",
    "    mstdbscan_folder,\n",
    ")\n",
    "\n",
    "clusterwholeperiod = results_whole_period['clusters']\n",
    "whole_survival = results_whole_period['survival']\n",
    "whole_time_bt_clusters = results_whole_period['time_bt_clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_survival.to_pickle(mstdbscan_folder/'whole_survival - 200m.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_whole_period_lausanne = analyze_period(\n",
    "    cases_lausanne,\n",
    "    'Whole period',\n",
    "    '2020/03/04-00:00:00',\n",
    "    '2022/04/15-00:00:00',\n",
    "    statpop_ha_gdf,\n",
    "    mstdbscan_folder\n",
    ")\n",
    "\n",
    "clusterwholeperiod_lausanne = results_whole_period_lausanne['clusters']\n",
    "whole_survival_lausanne = results_whole_period_lausanne['survival']\n",
    "whole_time_bt_clusters_lausanne = results_whole_period_lausanne['time_bt_clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_survival_lausanne.to_pickle(mstdbscan_folder/'whole_survival_lausanne - 200m.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Final data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_covariates = statpop_ha_gdf[['RELI','B20BTOT','E_KOORD','N_KOORD','geometry']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_covariates = gpd.GeoDataFrame(\n",
    "    gdf_covariates, geometry=gdf_covariates.geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### SES determinants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ses = pd.read_parquet(data_folder/'Socioeconomic determinants'/'df_ses_index.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_covariates = pd.merge(gdf_covariates, gdf_ses[['RELI','index_socio_class','index_socio_class_q3','index_socio_stand','index_socio_class_q3_inv']], on = 'RELI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Environmental determinants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_carnight = pd.read_pickle(data_folder/'Environmental determinants'/'ns_car_night.pkl')\n",
    "ns_carday = pd.read_pickle(data_folder/'Environmental determinants'/'ns_car_day.pkl')\n",
    "pm10 = pd.read_pickle(data_folder/'Environmental determinants'/'pm10_2020.pkl')\n",
    "pm25 = pd.read_pickle(data_folder/'Environmental determinants'/'pm25_2020.pkl')\n",
    "no2 = pd.read_pickle(data_folder/'Environmental determinants'/'no2_2020.pkl')\n",
    "lst = pd.read_pickle(data_folder/'Environmental determinants'/'gdf_lst.pkl')\n",
    "ndvi = pd.read_pickle(data_folder/'Environmental determinants'/'gdf_ndvi.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_columns = {\n",
    "    'LST': lst['mean_lst'],\n",
    "    'NDVI': ndvi['mean_ndvi'],\n",
    "    'pm10': pm10['mean'],\n",
    "    'pm25': pm25['mean'],\n",
    "    'no2': no2['mean'],\n",
    "    'noise_car_day': ns_carday['mean'],\n",
    "    'noise_car_night': ns_carnight['mean']\n",
    "}\n",
    "\n",
    "ch_statpop_ha = ch_statpop_ha.assign(**mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_covariates = pd.merge(gdf_covariates, ch_statpop_ha[['RELI','LST','NDVI','pm10','pm25','no2','noise_car_day','noise_car_night']], on ='RELI', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_covariates['pm25'] = gdf_covariates['pm25'].div(10)\n",
    "gdf_covariates['pm10'] = gdf_covariates['pm10'].div(10)\n",
    "gdf_covariates['no2'] = gdf_covariates['no2'].div(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "### Lagged population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnn8 = lps.weights.KNN(cKDTree(get_points_array(gdf_covariates.geometry)), 8)\n",
    "wnn24 = lps.weights.KNN(cKDTree(get_points_array(gdf_covariates.geometry)),24)\n",
    "wd_200 = lps.weights.DistanceBand(cKDTree(get_points_array(gdf_covariates.geometry)), 200)\n",
    "\n",
    "wnn8.transform = 'r'\n",
    "wnn24.transform = 'r'\n",
    "wd_200.transform = 'r'\n",
    "\n",
    "gdf_covariates['B20BTOT_lag8'] = lps.weights.lag_spatial(wnn8, gdf_covariates['B20BTOT'])\n",
    "gdf_covariates['B20BTOT_lag24'] = lps.weights.lag_spatial(wnn24, gdf_covariates['B20BTOT'])\n",
    "gdf_covariates['B20BTOT_lag200m'] = lps.weights.lag_spatial(wd_200, gdf_covariates['B20BTOT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Testing rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_covariates = pd.merge(gdf_covariates, df_testing_rates[df_testing_rates.period == 'full'][['RELI','testing_rate']], on = 'RELI', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_covariates['testing_rate'] = gdf_covariates['testing_rate'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "## Cox PH modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "from lifelines import (\n",
    "    CoxPHFitter,\n",
    "    ExponentialFitter,\n",
    "    KaplanMeierFitter,\n",
    "    LogLogisticFitter,\n",
    "    LogLogisticAFTFitter,\n",
    "    LogNormalFitter,\n",
    "    LogNormalAFTFitter,\n",
    "    WeibullFitter,\n",
    "    WeibullAFTFitter,\n",
    ")\n",
    "from lifelines.plotting import qq_plot\n",
    "from lifelines.utils import k_fold_cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Whole canton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the periods\n",
    "periods = ['first_period', 'second_period', 'third_period', 'fourth_period', 'fifth_period']\n",
    "\n",
    "# Create a list of dataframes, each with a different period\n",
    "gdf_covariates_periods = [gdf_covariates.assign(period=p) for p in periods]\n",
    "\n",
    "# Concatenate all dataframes\n",
    "gdf_covariates_period = pd.concat(gdf_covariates_periods, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_survival_period = pd.read_parquet(mstdbscan_folder/'combined_survival.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = whole_survival.groupby('RELI').agg({'Persistence_survival':'sum', 'status':'last'}).reset_index()\n",
    "df_period = whole_survival_period.groupby(['RELI','period']).agg({'Persistence_survival':'sum', 'status':'last'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covariates = pd.merge(df, gdf_covariates, on = 'RELI', how = 'right')\n",
    "df_period_covariates = pd.merge(df_period, gdf_covariates_period, on = ['RELI','period'], how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covariates['Persistence_survival'] = df_covariates['Persistence_survival'].fillna(0)\n",
    "df_covariates['status'] = df_covariates['status'].fillna(True)\n",
    "\n",
    "df_period_covariates['Persistence_survival'] = df_period_covariates['Persistence_survival'].fillna(0)\n",
    "df_period_covariates['status'] = df_period_covariates['status'].fillna(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Cox Proportional Hazards model\n",
    "cph = CoxPHFitter()   ## Instantiate the class to create a cph object\n",
    "cph.fit(df_covariates[df_covariates['index_socio_stand'].isnull()==False][['Persistence_survival','index_socio_stand','status']], robust=True, duration_col = 'Persistence_survival', event_col='status')   ## Fit the data to train the model\n",
    "cph.print_summary(2)    ## HAve a look at the significance of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "## Lausanne data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_survival_period_lausanne = pd.read_parquet(mstdbscan_folder/'combined_survival_lausanne.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lausanne = whole_survival_lausanne.groupby('RELI').agg({'Persistence_survival':'sum', 'status':'last'}).reset_index()\n",
    "df_period_lausanne = whole_survival_period_lausanne.groupby(['RELI','period']).agg({'Persistence_survival':'sum', 'status':'last'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lausanne_covariates = pd.merge(df_lausanne, gdf_covariates[gdf_covariates.RELI.isin(gdf_lausanne.RELI)], on = 'RELI', how = 'right')\n",
    "df_period_lausanne_covariates = pd.merge(df_period_lausanne, gdf_covariates_period[gdf_covariates_period.RELI.isin(gdf_lausanne.RELI)], on = ['RELI','period'], how = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lausanne_covariates['Persistence_survival'] = df_lausanne_covariates['Persistence_survival'].fillna(0)\n",
    "df_lausanne_covariates['status'] = df_lausanne_covariates['status'].fillna(True)\n",
    "\n",
    "df_period_lausanne_covariates['Persistence_survival'] = df_period_lausanne_covariates['Persistence_survival'].fillna(0)\n",
    "df_period_lausanne_covariates['status'] = df_period_lausanne_covariates['status'].fillna(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lausanne_covariates[df_lausanne_covariates['index_socio_stand'].isnull()==False][['Persistence_survival','index_socio_stand','status']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Cox Proportional Hazards model\n",
    "cph = CoxPHFitter()   ## Instantiate the class to create a cph object\n",
    "cph.fit(df_lausanne_covariates[df_lausanne_covariates['index_socio_stand'].isnull()==False][['Persistence_survival','index_socio_stand','status']], robust=True, duration_col = 'Persistence_survival', event_col='status')   ## Fit the data to train the model\n",
    "cph.print_summary(2)    ## HAve a look at the significance of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "## Save processed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covariates = gpd.GeoDataFrame(df_covariates, crs=2056)\n",
    "df_period_covariates = gpd.GeoDataFrame(df_period_covariates, crs=2056)\n",
    "df_lausanne_covariates = gpd.GeoDataFrame(df_lausanne_covariates, crs=2056)\n",
    "df_period_lausanne_covariates = gpd.GeoDataFrame(df_period_lausanne_covariates, crs=2056)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_covariates.to_parquet(data_folder/'df_survival_covariates.geoparquet')\n",
    "df_period_covariates.to_parquet(data_folder/'df_survival_period_covariates.geoparquet')\n",
    "\n",
    "df_lausanne_covariates.to_parquet(data_folder/'df_survival_covariates_lausanne.geoparquet')\n",
    "df_period_lausanne_covariates.to_parquet(data_folder/'df_survival_period_covariates_lausanne.geoparquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
