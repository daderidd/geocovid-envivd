{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set up environment variables and system paths\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "sys.path.append('./utils')\n",
    "\n",
    "# Third-party imports\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pysal\n",
    "import seaborn as sns\n",
    "import pysda\n",
    "import scipy\n",
    "import scipy.stats\n",
    "from scipy.stats import entropy\n",
    "from scipy import interpolate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from geofeather import from_geofeather, to_geofeather\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess as  sm_lowess\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import patches as mpatches\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.ops import nearest_points\n",
    "from tableone import TableOne\n",
    "\n",
    "# Local application/library specific imports\n",
    "import pyspace\n",
    "import utils\n",
    "\n",
    "\n",
    "# Pandas global settings\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# Matplotlib global styles\n",
    "mpl.rcParams['font.family'] = 'Avenir'\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "\n",
    "# Path configurations\n",
    "data_folder = Path('../data/')\n",
    "res_folder = Path('../results/')\n",
    "\n",
    "# DATA IMPORT placeholder\n",
    "# data_filename = input(\"Data file's name:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Municipalities - Load municipalities from the canton of Vaud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load communes polygon data\n",
    "gdf_communes = gpd.read_file(data_folder /'Administrative units'/ 'swissBOUNDARIES3D_1_3_TLM_HOHEITSGEBIET.shp', engine='pyogrio')\n",
    "\n",
    "# Filter for the Canton of Vaud (VD), excluding lake areas, and reset the index\n",
    "gdf_communes_vd = (gdf_communes[gdf_communes['KANTONSNUM'] == 22]\n",
    "               .query(\"NAME not in ['Lac Léman (VD)', 'Lac de Neuchâtel (VD)', 'Lac de Morat (VD)']\")\n",
    "               .dropna(subset=['geometry'])\n",
    "               .reset_index(drop=True))\n",
    "# Set the coordinate reference system (CRS)\n",
    "gdf_communes_vd = gdf_communes_vd.to_crs(2056)\n",
    "# Dissolve all municipalities into one for the canton\n",
    "gdf_communes_vd['dummy'] = 1\n",
    "gdf_canton_vd = gdf_communes_vd.dissolve(by='dummy', as_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Population data\n",
    "### Load inhabited hectares of the canton of Vaud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population statistics and create points GeoDataFrame\n",
    "df_statpop = pd.read_csv(data_folder / 'ag-b-00.03-vz2020statpop/STATPOP2020.csv', sep=';')\n",
    "geometry_points = [Point(xy) for xy in zip(df_statpop['E_KOORD'], df_statpop['N_KOORD'])]\n",
    "statpop_gdf_point = gpd.GeoDataFrame(df_statpop, crs=2056, geometry=geometry_points)\n",
    "\n",
    "# Spatial join with communes_vd and filter out non-intersecting points\n",
    "gdf_statpop_communes_vd_point = gpd.sjoin(statpop_gdf_point, gdf_communes_vd, predicate='intersects').drop('index_right', axis=1)\n",
    "\n",
    "# Create a square polygon around each point with size 100 x 100\n",
    "geometry_squares = [Polygon([(x, y), (x, y + 100), (x + 100, y + 100), (x + 100, y)]) for x, y in zip(gdf_statpop_communes_vd_point['E_KOORD'], gdf_statpop_communes_vd_point['N_KOORD'])]\n",
    "gdf_statpop_communes_vd_ha = gpd.GeoDataFrame(gdf_statpop_communes_vd_point, crs=2056, geometry=geometry_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_commune_ha = gdf_statpop_communes_vd_point.groupby('NAME').B20BTOT.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## COVID testing data\n",
    "### Load CHUV testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_covid_path = Path('/Users/david/Switchdrive/GEOCOVID/')\n",
    "df_covid = gpd.read_file(db_covid_path/'processed_data'/'COVID_3eme_round_archive'/'s3_20dfiltered_tests_geo.gpkg', engine='pyogrio')\n",
    "# Load toy COVID data\n",
    "# df_covid = pd.read_csv('../data/covid_data_example.csv')\n",
    "# df_covid = gpd.GeoDataFrame(df_covid, crs=4326, geometry=gpd.points_from_xy(df_covid.longitude, df_covid.latitude))\n",
    "# df_covid = gdf_covid.to_crs(2056)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Date processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date_reception' into standard formats and extract useful date components\n",
    "df_covid['date_reception'] = pd.to_datetime(df_covid['date_reception']).dt.strftime('%Y-%m-%d')\n",
    "df_covid['month'] = pd.to_datetime(df_covid['date_reception']).dt.strftime('%Y-%m')\n",
    "df_covid['week'] = pd.to_datetime(df_covid['date_reception']).dt.isocalendar().week\n",
    "df_covid['year'] = pd.to_datetime(df_covid['date_reception']).dt.isocalendar().year\n",
    "df_covid['week_str'] = df_covid.apply(\n",
    "    lambda row: datetime.datetime.strptime(f\"{row['year']}-{row['week']}-1\", \"%Y-%W-%w\"),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of individuals :', df_covid.id_patient_study2.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of tests :', df_covid.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Period definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time periods for analysis\n",
    "p1_start = '2020-01-10'\n",
    "p2_start = '2020-06-30'\n",
    "p3_start = '2020-12-16'\n",
    "p4_start = '2021-05-07'\n",
    "p5_start = '2021-11-28'\n",
    "p5_end = '2022-04-16'\n",
    "\n",
    "df_covid.loc[df_covid.date_reception.between(p1_start, p2_start), 'period'] = 'p1'\n",
    "df_covid.loc[df_covid.date_reception.between(p2_start, p3_start), 'period'] = 'p2'\n",
    "df_covid.loc[df_covid.date_reception.between(p3_start, p4_start), 'period'] = 'p3'\n",
    "df_covid.loc[df_covid.date_reception.between(p4_start, p5_start), 'period'] = 'p4'\n",
    "df_covid.loc[df_covid.date_reception.between(p5_start, p5_end), 'period'] = 'p5'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data(target_gdf, source_gdf, join_column, columns_to_map):\n",
    "    for col in columns_to_map:\n",
    "        target_gdf[col] = target_gdf[join_column].map(source_gdf.set_index(join_column)[col])\n",
    "    return target_gdf\n",
    "\n",
    "def process_covid_data(df_covid, gdf_statpop, gdf_communes):\n",
    "    gdf_covid_ha = gpd.sjoin(\n",
    "        gdf_statpop[['RELI', 'B20BTOT', 'NAME', 'EINWOHNERZ', 'geometry']],\n",
    "        df_covid[['id_demande_study2', 'date_reception', 'week', 'week_str', 'month', 'year', 'period', 'geometry']],\n",
    "        predicate='intersects',\n",
    "        how='right'\n",
    "    ).sort_values('B20BTOT', ascending=False)\n",
    "    \n",
    "    gdf_covid_ha = gdf_covid_ha.drop_duplicates(subset=['id_demande_study2'], keep='first').drop('index_left', axis=1)\n",
    "    gdf_covid_ha['RELI'] = gdf_covid_ha.apply(lambda row: utils.apply_min_dist(row, gdf_statpop), axis=1)\n",
    "    gdf_covid_ha['period'] = gdf_covid_ha['period'].str[1].fillna('0').astype(int)\n",
    "    \n",
    "    columns_to_map = ['B20BTOT', 'NAME', 'EINWOHNERZ']\n",
    "    gdf_covid_ha = map_data(gdf_covid_ha, gdf_statpop, 'RELI', columns_to_map)\n",
    "    gdf_covid_ha['pop_commune'] = gdf_covid_ha['NAME'].map(pop_commune_ha)\n",
    "    \n",
    "    return gdf_covid_ha\n",
    "\n",
    "# Main processing\n",
    "df_covid['res_cov_txt'] = df_covid['res_cov_txt'].str.upper()\n",
    "gdf_cases = df_covid[df_covid.res_cov_txt == 'POSITIVE']\n",
    "print('Total number of positive tests :', gdf_cases.shape[0])\n",
    "gdf_controls = df_covid[df_covid.res_cov_txt == 'NEGATIVE']\n",
    "print('Total number of negative tests :', gdf_controls.shape[0])\n",
    "\n",
    "# Process COVID data\n",
    "gdf_covid_ha = process_covid_data(df_covid, gdf_statpop_communes_vd_ha, gdf_communes_vd)\n",
    "gdf_cases_ha = process_covid_data(gdf_cases, gdf_statpop_communes_vd_ha, gdf_communes_vd)\n",
    "gdf_controls_ha = process_covid_data(gdf_controls, gdf_statpop_communes_vd_ha, gdf_communes_vd)\n",
    "\n",
    "# Calculate testing rates\n",
    "gdf_covid_ha_all_periods = pd.concat([\n",
    "    gdf_covid_ha,\n",
    "    gdf_covid_ha.assign(period='full')\n",
    "])\n",
    "\n",
    "df_testing_rates = gdf_covid_ha_all_periods.groupby(['RELI', 'period']).agg({\n",
    "    'id_demande_study2': 'size',\n",
    "    'B20BTOT': 'first'\n",
    "}).reset_index()\n",
    "df_testing_rates.columns = ['RELI', 'period', 'n_tests', 'population']\n",
    "df_testing_rates['testing_rate'] = (df_testing_rates['n_tests'] / df_testing_rates['population']) * 100\n",
    "\n",
    "# Save testing rates\n",
    "df_testing_rates.to_csv('../data/testing_rates.csv', index=False)\n",
    "\n",
    "# Update main GeoDataFrames with case and control counts\n",
    "for gdf in [gdf_statpop_communes_vd_point, gdf_statpop_communes_vd_ha]:\n",
    "    gdf['n_cases'] = gdf['RELI'].map(gdf_cases_ha.groupby('RELI').size()).fillna(0)\n",
    "    gdf['n_controls'] = gdf['RELI'].map(gdf_controls_ha.groupby('RELI').size()).fillna(0)\n",
    "    gdf['pop_commune'] = gdf['NAME'].map(pop_commune_ha)\n",
    "\n",
    "# Final spatial join for COVID communes\n",
    "gdf_covid_communes = gpd.sjoin(df_covid, gdf_communes_vd[['geometry', 'NAME']], predicate='intersects', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## MSTDBSCAN results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the MSTDBSCAN results\n",
    "mstdbscan_folder = data_folder / 'MSTDBSCAN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Load clusters of the whole period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clusters and points data from saved pickle files\n",
    "clusters_wholeperiod = pd.read_pickle(mstdbscan_folder / 'clusters_wholeperiod - 200m.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Load clusters of the combined periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters_seq = gpd.read_parquet(mstdbscan_folder/'combined_clusters.parquet')\n",
    "df_clusters_seq['mstDate'] = pd.to_datetime(df_clusters_seq['mstDate']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Associate cases to the MSTDBSCAN clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters_seq_wcases = gpd.sjoin(df_clusters_seq, gdf_cases[['date_reception', 'geometry']], predicate='intersects')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Calculate the number of cases by cluster persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter cases that match the MST date\n",
    "cases_in_clusters_byperiod = df_clusters_seq_wcases[df_clusters_seq_wcases.date_reception == df_clusters_seq_wcases.mstDate]\n",
    "cases_in_clusters_byperiod['Number of positive tests'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by clusterID and Strain and sum positive tests\n",
    "grouped = cases_in_clusters_byperiod.groupby(['clusterID'])\n",
    "n_cases_by_cluster_by_period = grouped['Number of positive tests'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the date difference for each group\n",
    "def date_diff(group):\n",
    "    max_date = group['mstTime'].max()\n",
    "    min_date = group['mstTime'].min()\n",
    "    return (max_date - min_date) + 1\n",
    "\n",
    "# Apply the date_diff function to each group and store the results in a new dataframe\n",
    "duration_by_cluster_by_period = grouped.apply(date_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert series to DataFrame and reset index\n",
    "duration_by_cluster_by_period = pd.DataFrame(duration_by_cluster_by_period, columns=['Cluster persistence (days)']).reset_index()\n",
    "n_cases_by_cluster_by_period = pd.DataFrame(n_cases_by_cluster_by_period).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes to combine number of cases and duration information\n",
    "df_combined_by_period = pd.merge(n_cases_by_cluster_by_period, duration_by_cluster_by_period, on=['clusterID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Same procedure but for MSTDBSCAN resulting from the analysis on the whole period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform spatial join between clusters and cases\n",
    "clusters_cases_join = gpd.sjoin(clusters_wholeperiod, gdf_cases[['date_reception', 'geometry']], predicate='intersects')\n",
    "\n",
    "# Convert dates to datetime format\n",
    "clusters_cases_join['mstDate'] = pd.to_datetime(clusters_cases_join['mstDate'])\n",
    "clusters_cases_join['date_reception'] = pd.to_datetime(clusters_cases_join['date_reception'])\n",
    "\n",
    "# Filter cases where reception date matches the MST date\n",
    "cases_in_clusters = clusters_cases_join[clusters_cases_join['date_reception'] == clusters_cases_join['mstDate']]\n",
    "cases_in_clusters['Number of positive tests'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by clusterID and calculate sum of positive tests\n",
    "n_cases_by_cluster = cases_in_clusters.groupby(['clusterID'])['Number of positive tests'].sum()\n",
    "\n",
    "# Apply the date_diff function to each group in the cluster data\n",
    "grouped_clusters = clusters_wholeperiod.groupby(['clusterID'])\n",
    "duration_by_cluster = grouped_clusters.apply(date_diff, include_groups=False)\n",
    "duration_by_cluster = pd.DataFrame(duration_by_cluster, columns=['Cluster persistence (days)']).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.merge(duration_by_cluster, n_cases_by_cluster, on = ['clusterID'], how = 'left').fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Prepare data for epidemic curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'date_reception' as the datetime index and resample to get weekly counts\n",
    "gdf_cases['date_reception'] = pd.to_datetime(gdf_cases['date_reception'])\n",
    "gdf_controls['date_reception'] = pd.to_datetime(gdf_controls['date_reception'])\n",
    "\n",
    "# Resample cases and controls to get weekly counts\n",
    "cases_by_week = gdf_cases.set_index('date_reception').resample('W').size().rename('Positive tests').to_frame()\n",
    "controls_by_week = gdf_controls.set_index('date_reception').resample('W').size().rename('Negative tests').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the weekly counts of positive and total tests, fill missing values with zero\n",
    "weekly_cases_controls = pd.merge(controls_by_week, cases_by_week, \n",
    "                            left_index=True, right_index=True, how='left').fillna(0)\n",
    "\n",
    "# Calculate the positive rate as a percentage\n",
    "weekly_cases_controls['Total tests'] = weekly_cases_controls['Positive tests'] + weekly_cases_controls['Negative tests']\n",
    "weekly_cases_controls['Positive rate'] = (weekly_cases_controls['Positive tests'] / weekly_cases_controls['Total tests'] * 100).round(1)\n",
    "weekly_cases_controls.index = pd.to_datetime(weekly_cases_controls.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Graphical abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = clusters_wholeperiod.sort_values(['clusterID','mstTime']).drop_duplicates(subset = 'clusterID', keep = 'last').plot(color = 'red', alpha = 0.2, figsize = (10,10), zorder = 3)\n",
    "gdf_canton_vd.geometry.boundary.plot(ax = ax, linewidth = 0.9, edgecolor = 'darkgrey')\n",
    "ax.set_axis_off()\n",
    "df_covid.plot(markersize = 0.5,color = 'black',alpha = 0.1, ax = ax,zorder = 1)\n",
    "scalebar = ScaleBar(1, units=\"m\", location=\"lower right\")\n",
    "ax.add_artist(scalebar)\n",
    "\n",
    "plt.savefig('../figures/Graphical abstract - Part A.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Assuming controls_by_week is correctly defined and includes 'Positive tests' and 'Total tests'\n",
    "weekly_cases_controls[['Positive tests', 'Total tests']].plot(legend=True, ax=ax)\n",
    "\n",
    "# Add vertical lines and labels for different periods\n",
    "periods = [('2020-03-02', '2020-03-10', 'First period'),\n",
    "           ('2020-06-30', '2020-07-18', 'Second period'),\n",
    "           ('2020-12-15', '2020-12-31', 'Third period'),\n",
    "           ('2021-05-07', '2021-06-12', 'Fourth period'),\n",
    "           ('2021-11-28', '2021-12-15', 'Fifth period')]\n",
    "\n",
    "for start, label_pos, label in periods:\n",
    "    ax.axvline(pd.to_datetime(start), color='black', linestyle=':', lw=2)\n",
    "    ax.text(pd.to_datetime(label_pos), 9500, label, color='#636363', fontsize=15)\n",
    "\n",
    "# Set labels and ticks\n",
    "ax.set_ylabel('Weekly count', labelpad=10, fontsize=14)\n",
    "ax.set_xlabel('Weeks', labelpad=10, fontsize=14)\n",
    "\n",
    "# X-ticks and grid settings\n",
    "xtick = pd.date_range(start=weekly_cases_controls.index.min(), end=weekly_cases_controls.index.max(), freq='W')\n",
    "ax.set_xticks(xtick, minor=True)\n",
    "ax.grid(True, which='minor', axis='x')\n",
    "ax.grid(True, which='major', axis='x')\n",
    "ax.set_ylim([0, 10000])\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('../figures/Figure 2.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin, ymin, xmax, ymax = gdf_communes[gdf_communes.NAME.isin(['Lausanne','Renens (VD)','Prilly','Ecublens (VD)','Pully','Le Mont-sur-Lausanne','Paudex','Lutry'])].total_bounds\n",
    "gdf_statpop_communes_laus_point = gdf_statpop_communes_vd_point.cx[xmin:xmax, ymin:ymax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_covid_communes_p1 = df_covid_communes[df_covid_communes.period == 'p1']\n",
    "# df_covid_communes_p2 = df_covid_communes[df_covid_communes.period == 'p2']\n",
    "# df_covid_communes_p3 = df_covid_communes[df_covid_communes.period == 'p3']\n",
    "# df_covid_communes_p4 = df_covid_communes[df_covid_communes.period == 'p4']\n",
    "# df_covid_communes_p5 = df_covid_communes[df_covid_communes.period == 'p5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lausanne_cases = gdf_statpop_communes_laus_point[['RELI','NAME','pop_commune','B20BTOT']].drop_duplicates()\n",
    "df_lausanne_controls = gdf_statpop_communes_laus_point[['RELI','NAME','pop_commune','B20BTOT']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2020, 3, 2)\n",
    "end_date = date(2022, 4, 15)\n",
    "\n",
    "delta = timedelta(days=1)\n",
    "while start_date <= end_date:\n",
    "    # print(str(start_date))\n",
    "    _df_cases = gdf_cases_ha[gdf_cases_ha.date_reception == str(start_date)]\n",
    "    _df_controls = gdf_cases_ha[gdf_cases_ha.date_reception == str(start_date)]\n",
    "    n_cases_ha = _df_cases.groupby('RELI').size()\n",
    "    n_controls_ha = _df_controls.groupby('RELI').size()\n",
    "    try:\n",
    "        df_lausanne_cases[start_date] =  df_lausanne_cases['RELI'].map(n_cases_ha).fillna(0)\n",
    "        df_lausanne_controls[start_date] =  df_lausanne_controls['RELI'].map(n_controls_ha).fillna(0)\n",
    "    except:\n",
    "        df_lausanne_cases[start_date] =  0\n",
    "        df_lausanne_controls[start_date] =  0\n",
    "    start_date += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lausanne_cases_stacked = (\n",
    "    df_lausanne_cases.set_index('RELI').filter(regex='2020|2021|2022', axis=1)\n",
    "    .stack()\n",
    "    .reset_index(name='cases')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack and reformat cases DataFrame\n",
    "df_lausanne_cases_stacked = (\n",
    "    df_lausanne_cases.set_index('RELI').filter(regex='2020|2021|2022', axis=1)\n",
    "    .stack()\n",
    "    .reset_index(name='cases')\n",
    ")\n",
    "df_lausanne_cases_stacked.columns = ['RELI', 'day', 'cases']\n",
    "df_lausanne_cases_stacked['day'] = pd.to_datetime(df_lausanne_cases_stacked['day']).dt.strftime('%Y-%m-%d')\n",
    "df_lausanne_cases_stacked['cases'] = df_lausanne_cases_stacked['cases'].astype(int)\n",
    "\n",
    "# Stack and reformat controls DataFrame\n",
    "df_lausanne_controls_stacked = (\n",
    "    df_lausanne_controls.set_index('RELI').filter(regex='2020|2021|2022', axis=1)\n",
    "    .stack()\n",
    "    .reset_index(name='controls')\n",
    ")\n",
    "df_lausanne_controls_stacked.columns = ['RELI', 'day', 'controls']\n",
    "df_lausanne_controls_stacked['day'] = pd.to_datetime(df_lausanne_controls_stacked['day']).dt.strftime('%Y-%m-%d')\n",
    "df_lausanne_controls_stacked['controls'] = df_lausanne_controls_stacked['controls'].astype(int)\n",
    "\n",
    "# Extract longitude and latitude from statpop_communes_vd_point\n",
    "geometry_crs = gdf_statpop_communes_vd_point.to_crs(epsg=4326)\n",
    "reli_lon = geometry_crs.set_index('RELI')['geometry'].x.to_dict()\n",
    "reli_lat = geometry_crs.set_index('RELI')['geometry'].y.to_dict()\n",
    "\n",
    "# Map coordinates to stacked dataframes\n",
    "df_lausanne_cases_stacked['lon'] = df_lausanne_cases_stacked['RELI'].map(reli_lon)\n",
    "df_lausanne_cases_stacked['lat'] = df_lausanne_cases_stacked['RELI'].map(reli_lat)\n",
    "df_lausanne_controls_stacked['lon'] = df_lausanne_controls_stacked['RELI'].map(reli_lon)\n",
    "df_lausanne_controls_stacked['lat'] = df_lausanne_controls_stacked['RELI'].map(reli_lat)\n",
    "\n",
    "# Merge the cases and controls dataframes\n",
    "df_lausanne_daily = pd.merge(\n",
    "    df_lausanne_controls_stacked, \n",
    "    df_lausanne_cases_stacked, \n",
    "    on=['RELI', 'day', 'lon', 'lat'], \n",
    "    how='outer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lausanne_daily['total'] = df_lausanne_daily[['cases','controls']].sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lausanne_daily_p1 = df_lausanne_daily[df_lausanne_daily.day.between(p1_start, p2_start)]\n",
    "df_lausanne_daily_p2 = df_lausanne_daily[df_lausanne_daily.day.between(p2_start, p3_start)]\n",
    "df_lausanne_daily_p3 = df_lausanne_daily[df_lausanne_daily.day.between(p3_start, p4_start)]\n",
    "df_lausanne_daily_p4 = df_lausanne_daily[df_lausanne_daily.day.between(p4_start, p5_start)]\n",
    "df_lausanne_daily_p5 = df_lausanne_daily[df_lausanne_daily.day.between(p5_start, p5_end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_covid_communes['time_since_start_epidemic'] = (pd.to_datetime(gdf_covid_communes['date_reception']) - pd.to_datetime('2020-01-10')).dt.days\n",
    "gdf_covid_communes['time_since_start_epidemic'] = pd.Categorical(gdf_covid_communes['time_since_start_epidemic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_covid_communes_positive = gdf_covid_communes[gdf_covid_communes.res_cov_txt == 'POSITIVE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peakedness_commune = pd.DataFrame(gdf_covid_communes_positive.groupby(['NAME','time_since_start_epidemic']).size().div(gdf_covid_communes_positive.groupby(['NAME']).size(), axis = 0), columns = ['frac_cases']).reset_index()\n",
    "df_peakedness_commune['time_since_start_epidemic'] = df_peakedness_commune['time_since_start_epidemic'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peakedness_commune_period = pd.DataFrame(gdf_covid_communes_positive.groupby(['NAME','period','time_since_start_epidemic']).size().div(gdf_covid_communes_positive.groupby(['NAME','period']).size(), axis = 0), columns = ['frac_cases']).reset_index()\n",
    "df_peakedness_commune_period['time_since_start_epidemic'] = df_peakedness_commune_period['time_since_start_epidemic'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_epidemic_density(df_peakedness_commune, city_name, x_text, y_text):\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    x = df_peakedness_commune[df_peakedness_commune.NAME == city_name]['time_since_start_epidemic']\n",
    "    y = df_peakedness_commune[df_peakedness_commune.NAME == city_name]['frac_cases']\n",
    "    sm_x, sm_y = sm_lowess(y, x, frac=0.05, it=15, return_sorted=True).T\n",
    "    plt.plot(sm_x, sm_y, color='#2b8cbe', linewidth=2)\n",
    "    ax.axvline(52, color='black', linestyle=':', lw=2)\n",
    "\n",
    "    ax.axvline(172, color='black', linestyle=':', lw=2)\n",
    "    ax.text(55, max(sm_y)*1.2, 'First period', color='#636363', fontsize=10)\n",
    "    \n",
    "    ax.axvline(341, color='black', linestyle=':', lw=2)\n",
    "    ax.text(180, max(sm_y)*1.2, 'Second period', color='#636363', fontsize=10)\n",
    "\n",
    "    ax.axvline(483, color='black', linestyle=':', lw=2)\n",
    "    ax.text(350, max(sm_y)*1.2, 'Third period', color='#636363', fontsize=10)\n",
    "\n",
    "    ax.axvline(688, color='black', linestyle=':', lw=2)\n",
    "    ax.text(520, max(sm_y)*1.2, 'Fourth period', color='#636363', fontsize=10)\n",
    "    \n",
    "    ax.axvline(827, color='black', linestyle=':', lw=2)\n",
    "    ax.text(700, max(sm_y)*1.2, 'Fifth period', color='#636363', fontsize=10)\n",
    "    ax.set_xlabel('Time since the start of the epidemic (days)')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_ylim(0, max(sm_y)*1.5)\n",
    "    ax.text(x_text, max(sm_y)*1.6, city_name, fontsize=12, color='black', ha='center', va='center')\n",
    "\n",
    "    plt.savefig(f'../figures/Figure 3A - Epidemic_density_{city_name}.png', dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_epidemic_density(df_peakedness_commune, 'Nyon',750, 0.014)\n",
    "plot_epidemic_density(df_peakedness_commune, 'Yverdon-les-Bains',720, 0.012)\n",
    "plot_epidemic_density(df_peakedness_commune, 'Montreux',80, 0.02)\n",
    "plot_epidemic_density(df_peakedness_commune, 'Lausanne',80, 0.006)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Inverse Shannon entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_entropy = {}\n",
    "for name in df_peakedness_commune.NAME.unique():\n",
    "    _peakedness = df_peakedness_commune[df_peakedness_commune.NAME == name]\n",
    "    entropy_value = entropy(_peakedness['frac_cases'], base=10)**-1\n",
    "    dict_entropy[name] = entropy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cases_commune = gdf_covid_communes_positive.groupby('NAME').id_demande_study2.nunique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_communes_vd['inv_shannon_entropy'] = gdf_communes_vd['NAME'].map(dict_entropy)\n",
    "gdf_communes_vd.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "gdf_communes_vd['n_cases'] = gdf_communes_vd['NAME'].map(dict_cases_commune)\n",
    "gdf_communes_vd['total_attack_rate'] = gdf_communes_vd['n_cases'].div(gdf_communes_vd['EINWOHNERZ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.regplot(data = gdf_communes_vd, lowess = True, x = 'inv_shannon_entropy', y = 'total_attack_rate')\n",
    "g.set(xscale=\"log\")\n",
    "g.set(yscale=\"log\")\n",
    "g.set_xlabel('Inverse Shannon entropy (log10)', size=12)\n",
    "g.set_ylabel('Final total attack rate (log10)', size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attack_rate_commune_period = pd.DataFrame(gdf_covid_communes_positive.groupby(['NAME','period']).size(), columns = ['n_cases']).reset_index()\n",
    "df_attack_rate_commune_period = pd.merge(df_attack_rate_commune_period, gdf_communes_vd[['NAME','EINWOHNERZ']], on='NAME', how = 'left')\n",
    "df_attack_rate_commune_period['total_attack_rate'] = df_attack_rate_commune_period['n_cases'].div(df_attack_rate_commune_period['EINWOHNERZ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy_by_period(group):\n",
    "    return entropy(group['frac_cases'], base=10)**-1\n",
    "\n",
    "# Group by 'NAME' and 'period'\n",
    "grouped = df_peakedness_commune_period.groupby(['NAME', 'period'])\n",
    "\n",
    "# Calculate entropy for each group and store results in a new DataFrame\n",
    "df_entropy_by_period = grouped.apply(calculate_entropy_by_period).reset_index()\n",
    "df_entropy_by_period.columns = ['NAME', 'period', 'inverse_shannon_entropy']\n",
    "df_entropy_by_period.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Convert the result DataFrame to a nested dictionary\n",
    "# dict_entropy = result_df.pivot(index='NAME', columns='period', values='entropy_value').to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entropy_and_attack_rate = pd.merge(df_entropy_by_period, df_attack_rate_commune_period[['NAME','period','n_cases','total_attack_rate']], on=['NAME','period'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_entropy_and_attack_rate['log_shannon'] =  np.log10(df_entropy_and_attack_rate['inverse_shannon_entropy'])\n",
    "df_entropy_and_attack_rate['log_attack_rate'] =  np.log10(df_entropy_and_attack_rate['total_attack_rate'])\n",
    "df_entropy_and_attack_rate[['inv_shannon_entropy_scaled']] = scaler.fit_transform(df_entropy_and_attack_rate[['inverse_shannon_entropy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entropy_and_attack_rate.groupby('period').inv_shannon_entropy_scaled.agg(['mean','std']).round(2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in df_entropy_and_attack_rate.period.unique():\n",
    "    _ = df_entropy_and_attack_rate[df_entropy_and_attack_rate.period == p]\n",
    "    print(p, _[['inv_shannon_entropy_scaled','log_attack_rate']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['inv_shannon_entropy_scaled']\n",
    "mytable = TableOne(df_entropy_and_attack_rate, columns, groupby='period', \n",
    "                   pval=True,\n",
    "                   htest_name=True, \n",
    "                   missing=True,\n",
    "                  decimals=2,\n",
    "                  normal_test=True,\n",
    "                  pval_adjust='bonferroni', \n",
    "                  nonnormal=columns,\n",
    "                  display_all=True,\n",
    "                  label_suffix = True)\n",
    "mytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(data=df_entropy_and_attack_rate, lowess=True, hue='period', x='log_shannon', y='log_attack_rate', scatter_kws={'alpha': 0.1})\n",
    "\n",
    "# Remove the default legend generated by Seaborn\n",
    "g._legend.remove()\n",
    "\n",
    "# Add a custom legend only for the lines\n",
    "line_labels = df_entropy_and_attack_rate['period'].str.replace('p','P').to_list()\n",
    "lines = g.axes[0][0].get_lines()\n",
    "legend = plt.legend(handles=lines, labels=line_labels, loc='upper right', title='Period')\n",
    "\n",
    "g.set_axis_labels(\"Inverse Shannon entropy (log10)\", \"Total attack rate (log10)\")\n",
    "plt.savefig('../figures/Figure 3B.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Lloyd's mean crowding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define highlighted communes\n",
    "highlight_communes = ['Lausanne', 'Nyon', 'Montreux', 'Yverdon-les-Bains']\n",
    "\n",
    "# Filter and select only relevant communes ensuring no null population sizes\n",
    "communes_four_main = gdf_communes_vd[gdf_communes_vd['NAME'].isin(highlight_communes) & gdf_communes_vd['EINWOHNERZ'].notnull()]\n",
    "\n",
    "# Calculate centroids for these communes directly, ensuring we don't modify the original DataFrame\n",
    "communes_four_main = communes_four_main.copy()\n",
    "communes_four_main['geometry'] = communes_four_main['geometry'].centroid\n",
    "\n",
    "# Extract x and y coordinates from geometry\n",
    "communes_four_main['x'], communes_four_main['y'] = communes_four_main.geometry.x, communes_four_main.geometry.y\n",
    "\n",
    "# Manually adjust coordinates for Lausanne to make sure the text is in the right place\n",
    "communes_four_main.loc[communes_four_main['NAME'] == 'Lausanne', ['x', 'y']] = [2539101, 1154000]\n",
    "\n",
    "# Recreate the geometry from the adjusted coordinates\n",
    "communes_four_main['geometry'] = gpd.points_from_xy(communes_four_main['x'], communes_four_main['y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map population data for each commune\n",
    "gdf_statpop_communes_vd_ha['pop_commune'] = gdf_statpop_communes_vd_ha['NAME'].map(pop_commune_ha)\n",
    "\n",
    "# Select relevant columns and remove duplicates\n",
    "df_lloyd = gdf_statpop_communes_vd_ha[['RELI', 'NAME', 'pop_commune', 'n_cases', 'n_controls', 'B20BTOT', 'geometry']].drop_duplicates()\n",
    "\n",
    "# Aggregate cases and controls by commune\n",
    "n_cases_commune = df_lloyd.groupby('NAME')['n_cases'].sum()\n",
    "n_controls_commune = df_lloyd.groupby('NAME')['n_controls'].sum()\n",
    "\n",
    "# Map the aggregate cases to health areas, filling missing values with zero\n",
    "df_lloyd['n_cases_ha'] = df_lloyd['RELI'].map(n_cases_ha).fillna(0)\n",
    "df_lloyd['n_cases_commune'] = df_lloyd['NAME'].map(n_cases_commune).fillna(0)\n",
    "\n",
    "# Calculate relative population and cases\n",
    "df_lloyd['relative_pop'] = df_lloyd['B20BTOT'] / df_lloyd['pop_commune']\n",
    "df_lloyd['relative_cases'] = df_lloyd['n_cases_ha'] / df_lloyd['n_cases_commune']\n",
    "\n",
    "# Compute the Hoover index\n",
    "# df_lloyd['hoover_index'] = (df_lloyd['relative_pop'] - df_lloyd['relative_cases']).abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'NAME' and calculate the crowding value in a vectorized way\n",
    "def calculate_crowding(group):\n",
    "    # Ensure the data needed for computation is passed explicitly\n",
    "    denominator = group['B20BTOT'].sum()\n",
    "    numerator = ((group['B20BTOT'] - 1) * group['B20BTOT']).sum()\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "# Apply the function to each group, ensuring grouping columns are not passed to the function\n",
    "# This change includes passing 'include_groups=False' to comply with future behavior of pandas\n",
    "dict_lloyd = df_lloyd.groupby('NAME', as_index=True).apply(calculate_crowding, include_groups=True).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map calculated crowding values to the geodataframe\n",
    "gdf_communes_vd['lloyd_mean_crowding'] = gdf_communes_vd['NAME'].map(dict_lloyd)\n",
    "\n",
    "# Calculate the logarithm of 'lloyd_mean_crowding', handling zero or negative values safely\n",
    "gdf_communes_vd['log_lloyd_mean_crowding'] = np.log10(gdf_communes_vd['lloyd_mean_crowding'].replace(0, np.nan))\n",
    "gdf_communes_vd['log_lloyd_mean_crowding'] = gdf_communes_vd['log_lloyd_mean_crowding'].fillna(0)\n",
    "\n",
    "# Calculate the logarithm of 'inv_shannon_entropy', ensuring non-positive values are handled\n",
    "gdf_communes_vd['log_inv_shannon_entropy'] = np.log10(gdf_communes_vd['inv_shannon_entropy'].clip(lower=1e-10))\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale 'inv_shannon_entropy' using MinMaxScaler and assign it back to the dataframe\n",
    "gdf_communes_vd['inv_shannon_entropy_scaled'] = scaler.fit_transform(gdf_communes_vd[['inv_shannon_entropy']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and an axis with specified size\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Plot the 'lloyd_mean_crowding' on the map using a color scale\n",
    "gdf_communes_vd.plot(\n",
    "    'lloyd_mean_crowding', \n",
    "    cmap='coolwarm', \n",
    "    missing_kwds={'color': 'lightgrey'},  # Color for missing values\n",
    "    legend=True, \n",
    "    legend_kwds={'label': 'Crowding', 'shrink': 0.5}, \n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "# Overlay highlighted communes with black points\n",
    "communes_four_main.geometry.plot(markersize=3, color='black', ax=ax)\n",
    "\n",
    "# Annotate communes with their names\n",
    "for idx, row in communes_four_main.iterrows():\n",
    "    ax.annotate(\n",
    "        text=row['NAME'], \n",
    "        xy=row.geometry.centroid.coords[0], \n",
    "        ha='left', \n",
    "        size=8\n",
    "    )\n",
    "\n",
    "# Remove axis for a cleaner look\n",
    "ax.set_axis_off()\n",
    "\n",
    "# Save the figure to a file\n",
    "plt.savefig('../figures/Figure 3C.png', dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "## Figure S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your color map\n",
    "cmap = 'coolwarm'\n",
    "\n",
    "# Normalize the color scale based on the minimum and maximum population values\n",
    "norm = plt.Normalize(vmin=gdf_statpop_communes_vd_ha[gdf_statpop_communes_vd_ha.NAME.isin(highlight_communes)]['B20BTOT'].min(), vmax=gdf_statpop_communes_vd_ha[gdf_statpop_communes_vd_ha.NAME.isin(highlight_communes)]['B20BTOT'].max())\n",
    "\n",
    "# Create a 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n",
    "\n",
    "# Flatten the axes to make it easier to iterate over them\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate over the desired periods (replace with your actual periods)\n",
    "for ax, commune in zip(axes, highlight_communes):\n",
    "    # Filter the data for the specific period\n",
    "    df_commune = gdf_statpop_communes_vd_ha[gdf_statpop_communes_vd_ha.NAME == commune]\n",
    "    \n",
    "    # Plot the population data\n",
    "    df_commune.plot(column='B20BTOT', cmap=cmap, norm=norm, missing_kwds={'color': 'lightgrey'}, ax=ax)\n",
    "    gdf_communes_vd[gdf_communes_vd.NAME==commune].geometry.boundary.plot(ax = ax, linewidth=0.6, color='black')\n",
    "\n",
    "    # Set the axis title and turn it off\n",
    "    ax.set_title(f'{commune}')\n",
    "    ax.set_axis_off()\n",
    "\n",
    "\n",
    "# Create a single colorbar for all subplots at the bottom center\n",
    "fig.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.8, wspace=0.2, hspace=0.2)\n",
    "cbar_ax = fig.add_axes([0.3, 0.95, 0.3, 0.02])  # Position and size of the colorbar axis\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])  # Only needed for an empty array for the scalar mappable\n",
    "cbar = fig.colorbar(sm, cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label('Population')\n",
    "# Display the maps\n",
    "plt.savefig('../figures/Figure S3.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for better readability\n",
    "gdf_communes_vd = gdf_communes_vd.rename(columns={\n",
    "    'log_inv_shannon_entropy': 'Inverse Shannon entropy (log10)',\n",
    "    'EINWOHNERZ': 'Population size',\n",
    "    'inv_shannon_entropy_scaled': 'Peakedness'\n",
    "})\n",
    "\n",
    "# Calculate area in square kilometers from geometry\n",
    "gdf_communes_vd['area_sq_km'] = gdf_communes_vd['geometry'].area / 1e6\n",
    "\n",
    "# Aggregate total area by commune and map it back to the original dataframe\n",
    "total_area_by_commune = gdf_communes_vd.groupby('NAME')['area_sq_km'].sum()\n",
    "gdf_communes_vd['total_area'] = gdf_communes_vd['NAME'].map(total_area_by_commune)\n",
    "\n",
    "# Calculate population density (inhabitants per square kilometer)\n",
    "gdf_communes_vd['Pop. density (hab/km2)'] = gdf_communes_vd['Population size'] / gdf_communes_vd['total_area']\n",
    "\n",
    "# Add a direct reference to population size for clarity (may be redundant)\n",
    "gdf_communes_vd['Pop. (hab)'] = gdf_communes_vd['Population size']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (12,12))\n",
    "g = sns.scatterplot(data = gdf_communes_vd, y = 'Inverse Shannon entropy (log10)', x = 'log_lloyd_mean_crowding',hue='Peakedness', size = 'Pop. density (hab/km2)', sizes=(10, 200), palette = 'RdYlGn_r')\n",
    "g.grid(True)\n",
    "g.set_ylabel('Inverse Shannon entropy (log10)', size=12)\n",
    "g.set_xlabel(\"Lloyd's mean crowding (log10)\", size=12)\n",
    "plt.savefig('../figures/Figure 4A.png', dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entropy_and_attack_rate = pd.merge(df_entropy_and_attack_rate, gdf_communes_vd[['NAME','geometry']], how = 'left', on= 'NAME')\n",
    "df_entropy_and_attack_rate = df_entropy_and_attack_rate.set_geometry('geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_period_entropy(df, period):\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    norm = plt.Normalize(vmin=0, vmax=1)\n",
    "\n",
    "    df_period = df[df.period == period]\n",
    "    \n",
    "    df_period.plot('inv_shannon_entropy_scaled', cmap='RdYlGn_r',norm=norm, missing_kwds={'color': 'lightgrey'}, legend=True,\n",
    "                   legend_kwds={'label': 'Epidemic peakedness', 'shrink': 0.5}, ax=ax)\n",
    "    ax.set_axis_off()\n",
    "    communes_four_main.geometry.plot(markersize = 2,color='black', ax = ax)\n",
    "    communes_four_main.apply(lambda x: ax.annotate(text=x.NAME, xy=x.geometry.centroid.coords[0], ha='left', size=6),axis=1)\n",
    "    plt.savefig(f'../figures/Figure 4B - Period {period}.png', dpi = 300, bbox_inches = 'tight')\n",
    "\n",
    "\n",
    "# Example usage\n",
    "plot_period_entropy(df_entropy_and_attack_rate, 'p1')\n",
    "plot_period_entropy(df_entropy_and_attack_rate, 'p2')\n",
    "plot_period_entropy(df_entropy_and_attack_rate, 'p3')\n",
    "plot_period_entropy(df_entropy_and_attack_rate, 'p4')\n",
    "plot_period_entropy(df_entropy_and_attack_rate, 'p5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "## Figure 5 - Hoover index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns to form the final DataFrame\n",
    "df_hoover = gdf_statpop_communes_vd_point[['RELI', 'NAME', 'B20BTOT', 'n_cases', 'n_controls', 'geometry']]\n",
    "\n",
    "# Group by 'NAME' and calculate sums for cases, controls, and population\n",
    "grouped_df_hoover = df_hoover.groupby('NAME').agg({\n",
    "    'n_cases': 'sum',\n",
    "    'n_controls': 'sum',\n",
    "    'B20BTOT': 'sum'\n",
    "})\n",
    "\n",
    "# Rename columns for clarity after aggregation\n",
    "grouped_df_hoover.rename(columns={'B20BTOT': 'pop_commune'}, inplace=True)\n",
    "\n",
    "# Map the aggregated data back to the original DataFrame\n",
    "df_hoover = df_hoover.merge(grouped_df_hoover, on='NAME', suffixes=('', '_commune'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate relative proportions\n",
    "df_hoover['relative_pop'] = df_hoover['B20BTOT'] / df_hoover['pop_commune']\n",
    "df_hoover['relative_cases'] = df_hoover['n_cases'] / df_hoover['n_cases_commune']\n",
    "df_hoover['relative_controls'] = df_hoover['n_controls'] / df_hoover['n_controls_commune']\n",
    "\n",
    "# Compute the Hoover index for each entry\n",
    "df_hoover['hoover_index'] = (df_hoover['relative_pop'] - df_hoover['relative_cases']).abs()\n",
    "\n",
    "# Sum the Hoover index by commune, multiply by 50, and sort\n",
    "hoover_index_full_period = df_hoover.groupby('NAME')['hoover_index'].sum().mul(50).sort_values()\n",
    "\n",
    "# Map the summed and scaled Hoover index back to the GeoDataFrame of communes\n",
    "gdf_communes_vd['hoover_index_wholeperiod'] = gdf_communes_vd['NAME'].map(hoover_index_full_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (10,10))\n",
    "gdf_communes_vd.plot('hoover_index_wholeperiod',cmap = 'Reds',scheme = 'quantiles',legend = True,ax = ax)\n",
    "ax.set_axis_off()\n",
    "ax.set_title(label = 'Hoover index by municipality - Full period')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "### Hoover index by week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "hoover_stats_weekly = []\n",
    "for week in gdf_cases_ha.week_str.astype(str).sort_values().unique():\n",
    "    df_cases = gdf_cases_ha[gdf_cases_ha.week_str == week]\n",
    "    n_cases_commune = df_cases.groupby('NAME').size()\n",
    "    n_cases_ha = df_cases.groupby('RELI').size()\n",
    "    df_hoover = gdf_statpop_communes_vd_point[['RELI','NAME','pop_commune','B20BTOT']].drop_duplicates()\n",
    "    df_hoover['n_cases_ha'] = df_hoover['RELI'].map(n_cases_ha).fillna(0)\n",
    "    df_hoover['n_cases_commune'] = df_hoover['NAME'].map(n_cases_commune).fillna(0)\n",
    "    df_hoover['relative_pop'] = df_hoover['B20BTOT']/df_hoover['pop_commune']\n",
    "    df_hoover['relative_cases'] = df_hoover['n_cases_ha']/df_hoover['n_cases_commune']\n",
    "    df_hoover['hoover_index'] = df_hoover['relative_pop'] - df_hoover['relative_cases']\n",
    "    df_hoover['hoover_index'] = df_hoover['hoover_index'].abs()\n",
    "    hoover_index_full_period = df_hoover.groupby('NAME')['hoover_index'].sum().mul(50)\n",
    "    gdf_communes_vd['hoover_index'] = gdf_communes_vd['NAME'].map(hoover_index_full_period)\n",
    "    hoover_stats_weekly.append({\n",
    "        'Week': week,\n",
    "        'mean': gdf_communes_vd['hoover_index'].mean(),\n",
    "        'std': gdf_communes_vd['hoover_index'].std(),\n",
    "        'min': gdf_communes_vd['hoover_index'].min(),\n",
    "        'max': gdf_communes_vd['hoover_index'].max(),\n",
    "        '_05': gdf_communes_vd['hoover_index'].quantile(0.05),\n",
    "        '_95': gdf_communes_vd['hoover_index'].quantile(0.95)\n",
    "    })\n",
    "    \n",
    "    \n",
    "    # fig,ax = plt.subplots(figsize = (10,10))\n",
    "    # gdf_communes_vd.plot('hoover_index',cmap = 'OrRd',legend = True,legend_kwds={'shrink': 0.6, 'label':'Locational Hoover index (%)'}, ax = ax)\n",
    "    # ctx.add_basemap(ax, source=ctx.providers.Stamen.Terrain,crs = 'EPSG:2056')\n",
    "    # ax.set_axis_off()\n",
    "    # ax.set_title(label = 'Hoover index - Week %s' % week)\n",
    "    # filename = str('Week %s' % week)\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig('../figures'/'Weekly'/filename, dpi = 100, bbox_inches = 'tight')\n",
    "    # plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "hoover_stats_weekly = pd.DataFrame(hoover_stats_weekly)\n",
    "hoover_stats_weekly['Week'] = pd.to_datetime(hoover_stats_weekly['Week'])\n",
    "hoover_stats_weekly = hoover_stats_weekly.rename(columns = {'mean':'Locational Hoover index (%)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "casebyweek = gdf_cases.groupby('week_str').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (12,6))\n",
    "hoover_stats_weekly.set_index('Week')['Locational Hoover index (%)'].plot(grid=True,legend = True,ax = ax)\n",
    "ax2=ax.twinx()\n",
    "casebyweek.plot(color = 'Red',legend = True, ax = ax2, label='Number of cases')\n",
    "ax2.set_ylabel(\"Number of cases\", labelpad=10,fontsize=14)\n",
    "ax.set_xlabel('Weeks', labelpad=10,fontsize=14)\n",
    "ax.set_ylabel('Locational Hoover index (%)', labelpad=10,fontsize=14)\n",
    "plt.xticks(rotation=45)\n",
    "ax.legend(loc='upper left', fontsize = 12)\n",
    "ax2.legend(loc='upper right', fontsize = 12)\n",
    "ax.axvline(pd.to_datetime('2020-03-02'), color='black', linestyle=':', lw=2)\n",
    "ax.text(pd.to_datetime('2020-03-20'), 80, 'First period', color='#636363', fontsize=15)\n",
    "\n",
    "ax.axvline(pd.to_datetime('2020-06-30'), color='black', linestyle=':', lw=2)\n",
    "ax.text(pd.to_datetime('2020-07-25'), 80, 'Second period', color='#636363', fontsize=15)\n",
    "\n",
    "ax.axvline(pd.to_datetime('2020-12-15'), color='black', linestyle=':', lw=2)\n",
    "ax.text(pd.to_datetime('2021-01-08'), 80, 'Third period', color='#636363', fontsize=15)\n",
    "ax.axvline(pd.to_datetime('2021-05-07'), color='black', linestyle=':', lw=2)\n",
    "ax.text(pd.to_datetime('2021-06-25'), 80, 'Fourth period', color='#636363', fontsize=15)\n",
    "\n",
    "ax.axvline(pd.to_datetime('2021-11-28'), color='black', linestyle=':', lw=2)\n",
    "ax.text(pd.to_datetime('2021-12-15'), 80, 'Fifth period', color='#636363', fontsize=15)\n",
    "# ax.axvline(pd.to_datetime('2020-03-16'), color='black', linestyle=':', lw=2, label='Lockdown period (March 16 - April 27 2020)')\n",
    "# ax.axvline(pd.to_datetime('2020-04-27'), color='black', linestyle=':', lw=2)\n",
    "# ax.axvline(pd.to_datetime('2020-11-06'), color='black', linestyle=':', lw=2, label='Second wave (November 6 - April 18 2021)')\n",
    "# ax.axvline(pd.to_datetime('2021-04-18'), color='black', linestyle=':', lw=2)\n",
    "ax.set_ylim([0,100])\n",
    "ax2.set_ylim([0,5000])\n",
    "\n",
    "xtick = pd.date_range( start=hoover_stats_weekly.Week.min( ), end=hoover_stats_weekly.Week.max( ), freq='W' )\n",
    "ax.set_xticks( xtick, minor=True )\n",
    "ax.grid('on', which='minor', axis='x' )\n",
    "ax.grid('off', which='major', axis='x')\n",
    "plt.savefig('../figures/Figure 5A', dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "### Hoover index by period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrame for storing Hoover statistics by period\n",
    "# stats_hoover_period = pd.DataFrame(columns=['Period', 'mean', 'std', 'min', 'max', '_05', '_95'])\n",
    "hoover_stats = []\n",
    "# Loop through unique periods sorted in order\n",
    "for p in sorted(gdf_cases_ha.period.unique()):\n",
    "    print(p)  # Optional: Print current processing period for tracking progress\n",
    "    _cases = gdf_cases_ha[gdf_cases_ha.period == p]\n",
    "\n",
    "    # Group and count cases by NAME and RELI\n",
    "    n_cases_commune = _cases.groupby('NAME').size()\n",
    "    n_cases_ha = _cases.groupby('RELI').size()\n",
    "\n",
    "    # Merge case data with geographical data\n",
    "    df_hoover = gdf_statpop_communes_vd_point[['RELI', 'NAME', 'pop_commune', 'B20BTOT']].drop_duplicates()\n",
    "    df_hoover['n_cases_ha'] = df_hoover['RELI'].map(n_cases_ha).fillna(0)\n",
    "    df_hoover['n_cases_commune'] = df_hoover['NAME'].map(n_cases_commune).fillna(0)\n",
    "\n",
    "    # Calculate relative population and cases\n",
    "    df_hoover['relative_pop'] = df_hoover['B20BTOT'] / df_hoover['pop_commune']\n",
    "    df_hoover['relative_cases'] = df_hoover['n_cases_ha'] / df_hoover['n_cases_commune']\n",
    "\n",
    "    # Compute Hoover index\n",
    "    df_hoover['hoover_index'] = (df_hoover['relative_pop'] - df_hoover['relative_cases']).abs()\n",
    "    hoover_index_full_period = df_hoover.groupby('NAME')['hoover_index'].sum().mul(50)\n",
    "\n",
    "    # Map Hoover index back to geographical DataFrame\n",
    "    gdf_communes_vd['hoover_index'] = gdf_communes_vd['NAME'].map(hoover_index_full_period)\n",
    "\n",
    "    # Collect statistics\n",
    "    hoover_stats.append({\n",
    "        'Period': p,\n",
    "        'mean': gdf_communes_vd['hoover_index'].mean(),\n",
    "        'std': gdf_communes_vd['hoover_index'].std(),\n",
    "        'min': gdf_communes_vd['hoover_index'].min(),\n",
    "        'max': gdf_communes_vd['hoover_index'].max(),\n",
    "        '_05': gdf_communes_vd['hoover_index'].quantile(0.05),\n",
    "        '_95': gdf_communes_vd['hoover_index'].quantile(0.95)\n",
    "    })\n",
    "    \n",
    "    # Visualization of Hoover index\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    norm = plt.Normalize(vmin=0, vmax=100)\n",
    "    gdf_communes_vd.plot(\n",
    "        'hoover_index', linewidth=0.1, edgecolor='grey', cmap='Blues',\n",
    "        norm=norm, missing_kwds={'color': 'lightgrey'}, legend=True,\n",
    "        legend_kwds={'shrink': 0.6, 'label': 'Locational Hoover index (%)'}, ax=ax\n",
    "    )\n",
    "\n",
    "    # Extra details and annotations\n",
    "    communes_four_main.geometry.plot(markersize=2, color='black', ax=ax)\n",
    "    communes_four_main.apply(lambda x: ax.annotate(text=x.NAME, xy=x.geometry.centroid.coords[0], ha='left', size=6), axis=1)\n",
    "    ax.set_axis_off()\n",
    "    filename = f'Period {p}'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../figures/Figure 5B - {filename}.png', dpi=300, bbox_inches='tight')\n",
    "# Create a DataFrame from the list of dictionaries containing stats\n",
    "stats_hoover_period = pd.DataFrame(hoover_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_hoover_period = stats_hoover_period.rename(columns = {'mean':'Locational Hoover index (%)'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "## Figure S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the figure and axis\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "# Plotting the 'Positive rate'\n",
    "weekly_cases_controls.plot(y='Positive rate', ax=ax, legend=False)\n",
    "\n",
    "# Setting x-axis ticks to weekly intervals\n",
    "xtick = pd.date_range(start=weekly_cases_controls.index.min(), end=weekly_cases_controls.index.max(), freq='W')\n",
    "ax.set_xticks(xtick, minor=True)\n",
    "ax.set_xticklabels([date.strftime('%Y-%m-%d') for date in xtick], rotation=45, ha='right')\n",
    "ax.grid('on', which='minor', axis='x' )\n",
    "ax.grid('off', which='major', axis='x')\n",
    "# Optional: enabling grid for minor x-axis ticks\n",
    "# ax.grid(True, which='minor', axis='x')\n",
    "\n",
    "# Setting labels and their properties\n",
    "ax.set_xlabel('Weeks', labelpad=10, fontsize=14)\n",
    "ax.set_ylabel('Positive rate (%)', labelpad=10, fontsize=14)\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig('../figures/Figure S2.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "## Figure S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "df_combined.plot.scatter('Number of positive tests','Cluster persistence (days)', logx=True, logy=True, legend=True, ax=ax, alpha = 0.5)\n",
    "ax.scatter([], [], label='MST-DBSCAN clusters', s=20)\n",
    "ax.legend(fontsize=12)\n",
    "plt.savefig('../figures/Figure S4.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation coefficient using pandas\n",
    "pearson_corr = df_combined['Number of positive tests'].corr(df_combined['Cluster persistence (days)'])\n",
    "\n",
    "# Calculate the Pearson correlation coefficient and the p-value using scipy\n",
    "r, p_value = scipy.stats.pearsonr(df_combined['Number of positive tests'], df_combined['Cluster persistence (days)'])\n",
    "\n",
    "print(\"Pearson correlation coefficient:\", pearson_corr)\n",
    "# print(\"Pearson correlation coefficient (scipy):\", pearson_corr_scipy)\n",
    "print(\"Associated p-value:\", p_value)\n",
    "\n",
    "\n",
    "# Define the confidence level, for example, 95%\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "\n",
    "# Calculate the Fisher's z transformation\n",
    "z = np.arctanh(r)\n",
    "\n",
    "# Calculate the standard error of z\n",
    "sample_size = len(df_combined)\n",
    "standard_error = 1 / np.sqrt(sample_size - 3)\n",
    "\n",
    "# Calculate the CI for the Fisher's z\n",
    "z_ci_lower = z - scipy.stats.norm.ppf(1 - alpha/2) * standard_error\n",
    "z_ci_upper = z + scipy.stats.norm.ppf(1 - alpha/2) * standard_error\n",
    "\n",
    "# Transform the CI back to the Pearson correlation coefficient\n",
    "r_ci_lower = np.tanh(z_ci_lower)\n",
    "r_ci_upper = np.tanh(z_ci_upper)\n",
    "\n",
    "print(\"Pearson correlation coefficient:\", round(r,2))\n",
    "print(\"Confidence interval (lower):\", round(r_ci_lower,2))\n",
    "print(\"Confidence interval (upper):\", round(r_ci_upper,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "## Figure 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate minimum and maximum dates for each cluster\n",
    "grouped_clusters_dates = clusters_wholeperiod.groupby('clusterID')['mstDate'].agg(['min', 'max']).reset_index()\n",
    "grouped_clusters_dates['min'] = pd.to_datetime(grouped_clusters_dates['min'])\n",
    "grouped_clusters_dates['max'] = pd.to_datetime(grouped_clusters_dates['max'])\n",
    "\n",
    "# Calculate the duration in days for each cluster\n",
    "grouped_clusters_dates['duration'] = (grouped_clusters_dates['max'] - grouped_clusters_dates['min']).dt.days + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark the start and end dates for clusters\n",
    "clusters_wholeperiod['startDate'] = pd.to_datetime(clusters_wholeperiod['mstDate'])\n",
    "clusters_wholeperiod['endDate'] = pd.to_datetime(clusters_wholeperiod['mstDate'])\n",
    "\n",
    "# Identify long-lasting clusters (duration of 30 days or more)\n",
    "long_clusters = clusters_wholeperiod[clusters_wholeperiod.clusterID.isin(\n",
    "    grouped_clusters_dates[grouped_clusters_dates.duration >= 30].clusterID\n",
    ")]\n",
    "\n",
    "# Simplify geometries for visualization or further analysis\n",
    "long_clusters['geometry'] = long_clusters['geometry'].simplify(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update cluster types to more generic terms\n",
    "long_clusters.loc[long_clusters['type'] == 'Directional growth', 'type'] = 'Growth'\n",
    "long_clusters.loc[long_clusters['type'] == 'Directional reduction', 'type'] = 'Reduction'\n",
    "long_clusters.loc[long_clusters['type'] == 'SplitMerge', 'type'] = 'Move'\n",
    "\n",
    "# Map colors to cluster types\n",
    "colors_cl = {\n",
    "    'Emerge': '#fb9a99', 'Growth': '#e31a1c', 'Steady': '#33a02c', 'Merge': '#ffffb3',\n",
    "    'Move': '#fdb462', 'Split': '#bebada', 'Reduction': '#1f78b4'\n",
    "}\n",
    "long_clusters['color'] = long_clusters['type'].map(colors_cl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert case reception dates to datetime\n",
    "gdf_cases['date_reception'] = pd.to_datetime(gdf_cases['date_reception'])\n",
    "\n",
    "# Assign a default color for cases visualization\n",
    "gdf_cases['color'] = 'red'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by start_date\n",
    "grouped_clusters_dates = grouped_clusters_dates.sort_values(by='clusterID', ascending=True)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "row_height = 0.5\n",
    "for idx, row in grouped_clusters_dates.iterrows():\n",
    "    start_date = row['min']\n",
    "    end_date = row['max']\n",
    "    cluster_id = row['clusterID']\n",
    "    ax.plot([start_date, end_date],[cluster_id, cluster_id] , lw=0.3, markersize=1.5, marker='o', color='grey', linestyle='-')\n",
    "\n",
    "for idx, row in grouped_clusters_dates[grouped_clusters_dates.duration >= 30].iterrows():\n",
    "    start_date = row['min']\n",
    "    end_date = row['max']\n",
    "    cluster_id = row['clusterID']\n",
    "    ax.plot([start_date, end_date],[cluster_id, cluster_id] , lw=0.6, markersize=3, marker='o', color='red', linestyle='-')\n",
    "    \n",
    "ax.axvline(pd.to_datetime('2020-03-02'), color='black', linestyle=':', lw=2)\n",
    "ax.text(pd.to_datetime('2020-03-20'), 3200, 'First period', color='#636363', fontsize=15)\n",
    "\n",
    "ax.axvline(pd.to_datetime('2020-06-30'), color='black', linestyle=':', lw=2)\n",
    "ax.text(pd.to_datetime('2020-07-25'), 3200, 'Second period', color='#636363', fontsize=15)\n",
    "\n",
    "ax.axvline(pd.to_datetime('2020-12-15'), color='black', linestyle=':', lw=2)\n",
    "ax.text(pd.to_datetime('2021-01-08'), 3200, 'Third period', color='#636363', fontsize=15)\n",
    "ax.axvline(pd.to_datetime('2021-05-07'), color='black', linestyle=':', lw=2)\n",
    "ax.text(pd.to_datetime('2021-06-25'), 3200, 'Fourth period', color='#636363', fontsize=15)\n",
    "\n",
    "ax.axvline(pd.to_datetime('2021-11-28'), color='black', linestyle=':', lw=2)\n",
    "ax.text(pd.to_datetime('2021-12-15'), 3200, 'Fifth period', color='#636363', fontsize=15)\n",
    "\n",
    "# Configure the axes and labels\n",
    "# ax.set_xlabel('Timeline')\n",
    "ax.set_ylabel('Cluster ID')\n",
    "xtick = pd.date_range( start=grouped_clusters_dates['min'].min( ), end=grouped_clusters_dates['max'].max( ), freq='W')\n",
    "ax.set_xticks(xtick, minor=True)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%Y'))\n",
    "# ax.xaxis.set_major_locator(mdates.DayLocator(interval=90))  # Change the interval for better readability if needed\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.savefig('../figures/Figure 6.png',dpi = 300, bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_clusters_dates.loc[grouped_clusters_dates['min'].between('2020-03-02','2020-06-30'), 'periodStart'] = 'p1'\n",
    "grouped_clusters_dates.loc[grouped_clusters_dates['min'].between('2020-07-01','2020-12-15'), 'periodStart'] = 'p2'\n",
    "grouped_clusters_dates.loc[grouped_clusters_dates['min'].between('2020-12-16','2021-05-07'), 'periodStart'] = 'p3'\n",
    "grouped_clusters_dates.loc[grouped_clusters_dates['min'].between('2021-05-08','2021-11-28'), 'periodStart'] = 'p4'\n",
    "grouped_clusters_dates.loc[grouped_clusters_dates['min'].between('2021-11-29','2022-04-15'), 'periodStart'] = 'p5'\n",
    "\n",
    "grouped_clusters_dates.loc[grouped_clusters_dates['max'].between('2020-03-02','2020-06-30'), 'periodEnd'] = 'p1'\n",
    "grouped_clusters_dates.loc[grouped_clusters_dates['max'].between('2020-07-01','2020-12-15'), 'periodEnd'] = 'p2'\n",
    "grouped_clusters_dates.loc[grouped_clusters_dates['max'].between('2020-12-16','2021-05-07'), 'periodEnd'] = 'p3'\n",
    "grouped_clusters_dates.loc[grouped_clusters_dates['max'].between('2021-05-08','2021-11-28'), 'periodEnd'] = 'p4'\n",
    "grouped_clusters_dates.loc[grouped_clusters_dates['max'].between('2021-11-29','2022-04-15'), 'periodEnd'] = 'p5'\n",
    "\n",
    "grouped_clusters_dates['period'] = grouped_clusters_dates['periodStart']+'-'+grouped_clusters_dates['periodEnd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_clusters_dates[grouped_clusters_dates.period.isin(['p1-p1','p2-p2','p3-p3','p4-p4','p5-p5'])].groupby('period')['duration'].agg(['median','std']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_clusters_dates = pd.merge(grouped_clusters_dates, df_combined, on = 'clusterID', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_clusters_dates[grouped_clusters_dates.period.isin(['p1-p1','p2-p2','p3-p3','p4-p4','p5-p5'])].groupby('period')['Number of positive tests'].agg(['median','std']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "## Figure S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight_communes = ['Lausanne','Montreux','Nyon','Yverdon-les-Bains']\n",
    "# communes_four_main = gdf_communes_vd[(gdf_communes_vd.NAME.isin(highlight_communes))&(gdf_communes_vd['Population size'].isnull() == False)]\n",
    "# communes_four_main['geometry'] = gdf_communes_vd[(gdf_communes_vd.NAME.isin(highlight_communes))&(gdf_communes_vd['Population size'].isnull() == False)].geometry.centroid\n",
    "\n",
    "# communes_four_main['x'], communes_four_main['y'] = communes_four_main.geometry.x, communes_four_main.geometry.y\n",
    "\n",
    "# communes_four_main.loc[9, 'x'] = 2539101\n",
    "# communes_four_main.loc[9, 'y'] = 1154000\n",
    "\n",
    "# communes_four_main['geometry'] = gpd.points_from_xy(communes_four_main.x, communes_four_main.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_parquet(data_folder/'df_survival_covariates.geoparquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patheffects import withStroke\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "df[['Persistence_survival','geometry']].plot('Persistence_survival', cmap = 'RdYlGn_r', legend=True, ax = ax, legend_kwds = {'shrink':0.4,'label': 'Cluster persistence (days)'}, zorder=2)\n",
    "ax.set_axis_off()\n",
    "gdf_communes_vd.plot(ax = ax, color='lightgrey', facecolor=None, zorder=1)\n",
    "communes_four_main.geometry.plot(markersize = 4,color='black', ax = ax)\n",
    "# communes_four_main.apply(lambda x: ax.annotate(text=x.NAME, xy=x.geometry.centroid.coords[0], ha='left', size=8),axis=1)\n",
    "for x, y, label in zip(communes_four_main.geometry.centroid.x, communes_four_main.geometry.centroid.y, communes_four_main['NAME']):\n",
    "    ax.text(x, y, label, fontsize=6, ha='right', va='bottom',\n",
    "            path_effects=[withStroke(linewidth=2, foreground='white')])\n",
    " \n",
    "plt.savefig('../figures/Figure S1.png',dpi=300, transparent=True, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
